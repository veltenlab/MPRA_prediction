{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Input, Masking, Lambda, concatenate, Bidirectional, Dense, Dropout, Flatten, Conv1D,BatchNormalization,  MaxPooling1D, Bidirectional, GRU, TimeDistributed, Concatenate\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(1337) # for reproducibility\n",
    "vocab = [\"A\", \"G\", \"C\", \"T\"]\n",
    "indices = tf.range(len(vocab), dtype = tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab,indices)\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, 1)\n",
    "\n",
    "\n",
    "batches = [\"LibA\",\"LibH\",\"LibB\",\"LibV\"] #batch keys used internally, not the same as in the manuscript\n",
    "bindices = tf.range(len(batches), dtype = tf.int64)\n",
    "btable_init = tf.lookup.KeyValueTensorInitializer(batches,bindices)\n",
    "btable = tf.lookup.StaticVocabularyTable(btable_init, 1)\n",
    "\n",
    "record_defaults = [\n",
    "    tf.constant([''], dtype=tf.string),\n",
    "    tf.constant([''], dtype=tf.string),\n",
    "    tf.constant([''], dtype=tf.string),\n",
    "    tf.constant([''], dtype=tf.string),\n",
    "    tf.constant([''], dtype=tf.string),  \n",
    "    tf.constant([''], dtype=tf.string),\n",
    "    tf.constant([''], dtype=tf.string),\n",
    "    tf.constant([''], dtype=tf.string),  \n",
    "    tf.constant([''], dtype=tf.string),\n",
    "    tf.constant([''], dtype=tf.string),\n",
    "]\n",
    "\n",
    "# Nadav dataset\n",
    "\n",
    "def data_reader(file, batch_size=100, n_parse_threads=8):\n",
    "    \"\"\"Method for reading the data in an optimized way, can be used inside model.fit()\n",
    "    \n",
    "    Args:\n",
    "        file (_type_): path to csv file\n",
    "        batch_size (int, optional): _description_. Defaults to 100.\n",
    "        n_parse_threads (int, optional): _description_. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "        dataset.batch: batch dataset object \n",
    "    \"\"\"\n",
    "    dataset = tf.data.TextLineDataset(file).skip(1)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "def preprocess(record):\n",
    "    \"\"\"Preprocessing method of a dataset object, one-hot-encodes the data\n",
    "\n",
    "    Args:\n",
    "        record (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        X (2D np.array): one-hot-encoded input sequence\n",
    "        Y (1D np.array): MPRA measurements for each cell state\n",
    "    \"\"\"\n",
    "    # Get fields from the data\n",
    "    fields = tf.io.decode_csv(record, record_defaults=record_defaults)\n",
    "    \n",
    "    # One-hot-encode data\n",
    "    chars = tf.strings.bytes_split(fields[0])\n",
    "    chars_indeces = table.lookup(chars)\n",
    "    batch_indeces = btable.lookup(fields[2])\n",
    "    \n",
    "    X = tf.one_hot(chars_indeces, depth = len(vocab))\n",
    "    B = tf.one_hot(batch_indeces, depth = len(batches))\n",
    "    # Combine y for each cell type into one vector \n",
    "    Y = tf.stack(fields[3:])\n",
    "    \n",
    "    # Replace missing values with -1\n",
    "    Y= tf.where(tf.equal(Y,  \"nan\"), [\"-1\"], Y)\n",
    "    Y= tf.where(tf.equal(Y,  \"NA\"), [\"-1\"], Y)\n",
    "    Y = tf.strings.to_number(Y, tf.float32)\n",
    "    return (X,B),Y\n",
    "\n",
    "# Get first item of the dataset to get the shape of the input data\n",
    "for element in data_reader(\"data.all/complete_data.csv\"):\n",
    "    input_shape = element[0][0].shape\n",
    "    output_shape = element[1].shape\n",
    "    batch_shape = element[0][1].shape\n",
    "    \n",
    "print(output_shape)\n",
    "print(input_shape)\n",
    "print(batch_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we initialize the df where each fold test prediction will be appended to\n",
    "# the list containing the correlations of each fold is also initialized\n",
    "corr_list = []\n",
    "\n",
    "# We define a custom normalization layer to then compile on the model\n",
    "class CustomNormalization(Layer):\n",
    "    \"\"\"Custom normalization layer that normalizes the output of the neural network\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Add trainable variables for mean and standard deviation\n",
    "        self.mean = self.add_weight(\"mean\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        self.stddev = self.add_weight(\"stddev\", shape=(1,), initializer=\"ones\", trainable=True)\n",
    "        super(CustomNormalization, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Normalize the inputs using the learned mean and standard deviation\n",
    "        return (inputs - self.mean) / (self.stddev + 1e-8)\n",
    "\n",
    "# We define the method to compute the pearson correlation between prediction and ground truth in the multi_head case\n",
    "def pearson_correlation_multi_head(predictions, ground_truth, mask_value=-1):\n",
    "    \"\"\"Computes Pearson Correlation between predictions and ground truth for each column\n",
    "    Args:\n",
    "        predictions (np.array): 2D array of prediction values (N, 7)\n",
    "        ground_truth (np.array): 2D array of ground truth values (N, 7)\n",
    "        mask_value (float): Value in ground truth to be ignored in correlation computation\n",
    "\n",
    "    Returns:\n",
    "        correlations (np.array): 1D array of Pearson correlations for each column\n",
    "    \"\"\"\n",
    "    # Ensure predictions and ground_truth have the same shape\n",
    "    assert predictions.shape == ground_truth.shape, \"Input shapes do not match\"\n",
    "\n",
    "    n_columns = predictions.shape[1]\n",
    "    correlations = np.zeros(n_columns)\n",
    "\n",
    "    for col in range(n_columns):\n",
    "        x = predictions[:, col]\n",
    "        y = ground_truth[:, col]\n",
    "\n",
    "        # Exclude values in ground truth equal to mask_value\n",
    "        valid_indices = (y != mask_value)\n",
    "        x = x[valid_indices]\n",
    "        y = y[valid_indices]\n",
    "\n",
    "        if len(x) == 0 or len(y) == 0:\n",
    "            # If no valid values, set correlation to NaN\n",
    "            correlations[col] = np.nan\n",
    "        else:\n",
    "            # Calculate mean of x and y\n",
    "            mean_x = np.mean(x)\n",
    "            mean_y = np.mean(y)\n",
    "\n",
    "            # Calculate the numerator and denominators of the correlation coefficient\n",
    "            numerator = np.sum((x - mean_x) * (y - mean_y))\n",
    "            denominator_x = np.sqrt(np.sum((x - mean_x) ** 2))\n",
    "            denominator_y = np.sqrt(np.sum((y - mean_y) ** 2))\n",
    "\n",
    "            # Calculate the correlation coefficient\n",
    "            correlation = numerator / (denominator_x * denominator_y)\n",
    "            correlations[col] = correlation\n",
    "\n",
    "    return correlations\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Define plotting function of loss\n",
    "def create_plots(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a758548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom loss function\n",
    "class MaskedMSE(tf.keras.losses.Loss):\n",
    "    \"\"\"Computes the MSE loss and prevents missing values backpropagation (previously replaced by -1.0)\n",
    "\n",
    "    Args:\n",
    "        tf (_type_): _description_\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_value=-1, **kwargs):\n",
    "        super(MaskedMSE, self).__init__(**kwargs)\n",
    "        self.mask_value = mask_value\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Create a mask for valid elements (not equal to the specified mask_value)\n",
    "        mask = tf.math.not_equal(y_true, self.mask_value)\n",
    "\n",
    "        # Compute MSE loss only for valid elements\n",
    "        loss = tf.where(mask, tf.square(y_true - y_pred), 0.0)\n",
    "\n",
    "        # Calculate the mean loss\n",
    "        mean_loss = tf.reduce_sum(loss) / tf.reduce_sum(tf.cast(mask, dtype=tf.float32))\n",
    "\n",
    "        return mean_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0250f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "corr_coefficients = {}\n",
    "\n",
    "df_test_10folds  = pd.DataFrame(columns=['State_3E',\n",
    "                                         \"seq\",\n",
    "                                         \"avg_prediction_State_1M\",\n",
    "                                         \"avg_prediction_State_2D\",\n",
    "                                         \"avg_prediction_State_3E\",\n",
    "                                         \"avg_prediction_State_4M\",\n",
    "                                         \"avg_prediction_State_5M\",\n",
    "                                         \"avg_prediction_State_6N\",\n",
    "                                         \"avg_prediction_State_7M\"])\n",
    "\n",
    "# We iterate through each of the train folds to train, test and validate the model\n",
    "\n",
    "for i in range(1,11):\n",
    "    #Define inputs\n",
    "    input_path_train = \"data.all/all_train_\"+str(i)+\".csv\"\n",
    "    input_path_valid = \"data.all/all_valid_\"+str(i)+\".csv\"\n",
    "    input_path_test = \"data.all/all_test_\"+str(i)+\".csv\"\n",
    "    \n",
    "    # Read test data to then predict\n",
    "    df_test = pd.read_csv(input_path_test)\n",
    "    df_test[\"fold\"] = str(i)\n",
    "    \n",
    "    predictions_sum = np.zeros((df_test.shape[0], 7))\n",
    "    \n",
    "    correlation_ensemble = []\n",
    "    # Initialize vector containing predictions\n",
    "    \n",
    "    for j in range(1,11):\n",
    "        \n",
    "        tf.random.set_seed(j)\n",
    "        \n",
    "        # Define and compile model\n",
    "        inputs = Input(shape=(input_shape[1],input_shape[2]), name=\"inputs\")\n",
    "        batchinput = Input(shape=(batch_shape[1]))\n",
    "        layer = Conv1D(250, kernel_size=7, strides=1, activation='relu', name=\"conv1\")(inputs)  # 250 7 relu\n",
    "        layer = Dropout(0.5)(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Conv1D(250, 8, strides=1, activation='softmax', name=\"conv2\")(layer)  # 250 8 softmax\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = MaxPooling1D(pool_size=2, strides=None, name=\"maxpool1\")(layer)\n",
    "        layer = Dropout(0.3)(layer)\n",
    "        layer = Conv1D(250, 3, strides=1, activation='softmax', name=\"conv3\")(layer)  # 250 3 softmax\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Dropout(0.5)(layer)\n",
    "        layer = Conv1D(100, 2, strides=1, activation='softmax', name=\"conv4\")(layer)  # 100 3 softmax\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = MaxPooling1D(pool_size=1, strides=None, name=\"maxpool2\")(layer)\n",
    "        layer = Dropout(0.5)(layer)\n",
    "        layer = Flatten()(layer)\n",
    "        layer = Concatenate()([layer, batchinput])\n",
    "        layer = Dense(300, activation='sigmoid')(layer)  # 300\n",
    "        layer = Dropout(0.5)(layer)\n",
    "        layer = Dense(200, activation='sigmoid')(layer)  # 300\n",
    "        predictions = Dense(7, activation='linear')(layer)\n",
    "        norm_predictions = CustomNormalization()(predictions)  # Assuming \"predictions\" is your existing output\n",
    "\n",
    "        model = Model(inputs=[inputs,batchinput], outputs=norm_predictions)\n",
    "        model.summary()\n",
    "\n",
    "        # compile model\n",
    "        model.compile(optimizer= Adam(clipvalue=1.0),  # You can adjust the clipvalue as needed\n",
    "                    loss=MaskedMSE(mask_value=-1),\n",
    "                    metrics=[\"mse\"]\n",
    "                    )\n",
    "\n",
    "        # Run model\n",
    "        history=model.fit(data_reader(input_path_train, batch_size=100),\n",
    "                                epochs=20,\n",
    "                                validation_data=data_reader(input_path_valid,batch_size=500),\n",
    "                                callbacks=None,\n",
    "                                verbose=1)    \n",
    "\n",
    "        model_path = \"weights.all/fold_\"+str(i)+\"_ens_\"+str(j)+\".h5\"\n",
    "        model.save_weights(model_path, save_format='h5') \n",
    "        #model.load_weights(model_path)\n",
    "        \n",
    "        # We predict the test data\n",
    "        predicted = model.predict(data_reader(input_path_test, batch_size=500))\n",
    "\n",
    "        # Sum the predicted values (will be averaged at the end of the 10 iterations)\n",
    "        predictions_sum =  np.add(predictions_sum, predicted)\n",
    "        \n",
    "        \n",
    "        # We reed the data in the same order to compute the correlation score\n",
    "        test_data = data_reader(input_path_test,batch_size=500)\n",
    "        test_tensor = np.empty(shape=[0,7])\n",
    "        for batch in test_data:\n",
    "            test_tensor = np.append(test_tensor, batch[1], axis=0)\n",
    "            \n",
    "        # Append correlation coefficient and append to previous\n",
    "        corr_coefficient = pearson_correlation_multi_head(test_tensor, predicted, mask_value=-1)\n",
    "        correlation_ensemble.append(corr_coefficient)\n",
    "        avg_corr_coefficient = 1\n",
    "        \n",
    "        \n",
    "    # We fill the dataframe with predictions and fold annotation\n",
    "    \n",
    "    predicted = predictions_sum/10 # We divide by the number of iterations\n",
    "    \n",
    "    df_test[\"avg_prediction_State_1M\"] = predicted[:,0]\n",
    "    df_test[\"avg_prediction_State_2D\"] = predicted[:,1]\n",
    "    df_test[\"avg_prediction_State_3E\"] = predicted[:,2]\n",
    "    df_test[\"avg_prediction_State_4M\"] = predicted[:,3]\n",
    "    df_test[\"avg_prediction_State_5M\"] = predicted[:,4]\n",
    "    df_test[\"avg_prediction_State_6N\"] = predicted[:,5]\n",
    "    df_test[\"avg_prediction_State_7M\"] = predicted[:,6]\n",
    "    df_test[\"fold\"] = str(i)\n",
    "    df_test[\"partition\"] = \"test\"\n",
    "    corr_coefficients[\"fold\"+str(i)] = avg_corr_coefficient\n",
    "\n",
    "    \n",
    "    # Append fold to previous folds\n",
    "    df_test_10folds = pd.concat([df_test_10folds, df_test], ignore_index=True)    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Save the results for all folds\n",
    "df_test_10folds.to_csv(\"predictions_all.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "df_test_10folds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
