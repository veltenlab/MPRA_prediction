{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ae6861",
   "metadata": {},
   "source": [
    "# MPRA regression with K-fold cross validation\n",
    "\n",
    "### Environment \n",
    "The next chunk contains the commands necessary to install the environment required to run this jupyter notebook\n",
    "Skip this chunk if the installation was previously done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "conda create --name tf_MPRA python=3.9.7\n",
    "conda activate tf_MPRA\n",
    "pip install tensorflow[and-cuda]\n",
    "conda install -c anaconda ipykernel \n",
    "conda install -c anaconda pandas\n",
    "conda install -c anaconda numpy\n",
    "conda install -c anaconda scikit-learn \n",
    "conda install -c conda-forge matplotlib\n",
    "\n",
    "# After installation if you are using VSCODE to run the notebook you have to close it and re-open\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a1d3c9",
   "metadata": {},
   "source": [
    "### Library imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff43395b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Layer, Lambda, concatenate, Bidirectional, Dense, Dropout, Flatten, Conv1D,BatchNormalization,  MaxPooling1D, Bidirectional, GRU, TimeDistributed\n",
    "from tensorflow.keras.layers import Normalization\n",
    "from tensorflow.keras.initializers import Constant\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b63fd8",
   "metadata": {},
   "source": [
    "### Input ingestion\n",
    "\n",
    "Here we define the methods to read and ingest data and we initialize the random seed.\n",
    "\n",
    "Since we are processing the entire sequence the vocabulary is comprised of upper case nucleotides\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368fda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337) # for reproducibility\n",
    "\n",
    "# Lower case vocabulary\n",
    "vocab = [\"A\", \"G\", \"C\", \"T\"]\n",
    "\n",
    "# These are the defaults of the data reader method \n",
    "# (each column in the ingested csv must be initialized with the right data type, otherwise the data ingestion fails )\n",
    "indices = tf.range(len(vocab), dtype = tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab,indices)\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, 1)\n",
    "defs = [0.] * 1 + [tf.constant([], dtype = \"string\")]\n",
    "\n",
    "# Nadav dataset\n",
    "\n",
    "def data_reader(file, batch_size=100, n_parse_threads=4):\n",
    "    \"\"\"Method for reading the data in an optimized way, can be used inside model.fit()\n",
    "    \n",
    "    Args:\n",
    "        file (_type_): path to csv file\n",
    "        batch_size (int, optional): _description_. Defaults to 100.\n",
    "        n_parse_threads (int, optional): _description_. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "        dataset.batch: batch dataset object \n",
    "    \"\"\"\n",
    "    dataset = tf.data.TextLineDataset(file).skip(1)\n",
    "    dataset=dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "def preprocess(record):\n",
    "    \"\"\"Preprocessing method of a dataset object, one-hot-encodes the data\n",
    "\n",
    "    Args:\n",
    "        record (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        X (2D np.array): one-hot-encoded input sequence\n",
    "        Y (1D np.array): MPRA measurements\n",
    "\n",
    "    \"\"\"\n",
    "    fields = tf.io.decode_csv(record, record_defaults=defs)\n",
    "    chars = tf.strings.bytes_split(fields[1])\n",
    "    chars_indeces = table.lookup(chars)\n",
    "    X = tf.one_hot(chars_indeces, depth = len(vocab))\n",
    "    Y = fields[0]\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41502a80",
   "metadata": {},
   "source": [
    "### k-fold cross validation split\n",
    "Here we take the initial csv file and we split it in 3 partitions k times\n",
    "\n",
    "It is possible to randomize the sequences and augment, since the masking of the model motifs was a better choice\n",
    "for understanding the background this strategy is here commented out and not used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6395a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS VALIDATION (10 fold)\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Split the data in three partitions\n",
    "whole_data = pd.read_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/LibA_wide_pivot_state3.csv\")\n",
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 2008)\n",
    "\n",
    "o=1\n",
    "# For each fold we split again to get the third partition\n",
    "for i in kf.split(whole_data):\n",
    "    # Get train/test split and upper case all nucleotides\n",
    "    train = whole_data.iloc[i[0]]\n",
    "    train[\"seq\"] = train['seq'].str.upper() \n",
    "    \n",
    "    test =  whole_data.iloc[i[1]]\n",
    "    test[\"seq\"] = test['seq'].str.upper() \n",
    "\n",
    "    train, validation = train_test_split(train, test_size=0.11, random_state=42)\n",
    "    \n",
    "    train.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(o)+\"_LibA_wide_pivot_state3_train.csv\", index=False)\n",
    "    test.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(o)+\"_LibA_wide_pivot_state3_test.csv\", index=False)\n",
    "    validation.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(o)+\"_LibA_wide_pivot_state3_validation.csv\", index=False)\n",
    "    o+=1\n",
    "\n",
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe7928e",
   "metadata": {},
   "source": [
    "### Deep Learning model\n",
    "\n",
    "Here we run the model which is based on this paper : \n",
    "\n",
    "https://doi.org/10.1101/2023.03.05.531189\n",
    "\n",
    "I have added a Normalization layer parametrized with two parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_test_10folds  = pd.DataFrame(columns=['State_3E', \"seq\", \"prediction\"])\n",
    "corr_list = []\n",
    "\n",
    "# We define a custom normalization layer to then compile on the model\n",
    "class CustomNormalization(Layer):\n",
    "    \"\"\"Custom normalization layer that normalizes the output of the neural network\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Add trainable variables for mean and standard deviation\n",
    "        self.mean = self.add_weight(\"mean\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        self.stddev = self.add_weight(\"stddev\", shape=(1,), initializer=\"ones\", trainable=True)\n",
    "        super(CustomNormalization, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Normalize the inputs using the learned mean and standard deviation\n",
    "        return (inputs - self.mean) / (self.stddev + 1e-8)\n",
    "\n",
    "# We define the method to compute the pearson correlation between prediction and ground truth\n",
    "def pearson_correlation(x, y):\n",
    "    \"\"\"Computes Pearson Correlation between x and y\n",
    "    Args:\n",
    "        x (np.array): vector of predictions values\n",
    "        y (np.array): vector of ground truth values\n",
    "\n",
    "    Returns:\n",
    "        (float): pearson correlation\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    # Calculate the mean of x and y\n",
    "    mean_x = sum(x) / n\n",
    "    mean_y = sum(y) / n\n",
    "    \n",
    "    # Calculate the numerator and denominators of the correlation coefficient\n",
    "    numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n",
    "    denominator_x = math.sqrt(sum((xi - mean_x) ** 2 for xi in x))\n",
    "    denominator_y = math.sqrt(sum((yi - mean_y) ** 2 for yi in y))\n",
    "    \n",
    "    # Calculate the correlation coefficient\n",
    "    correlation = numerator / (denominator_x * denominator_y)\n",
    "    return correlation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Define plotting function of loss\n",
    "def create_plots(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada1f870",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "Here we iterate through the folds and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4155f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(1,11):\n",
    "    \n",
    "    # Define inputs\n",
    "    input_path_train = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(i)+\"_LibA_wide_pivot_state3_train.csv\"\n",
    "    input_path_valid = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(i)+\"_LibA_wide_pivot_state3_validation.csv\"\n",
    "    input_path_test = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(i)+\"_LibA_wide_pivot_state3_test.csv\"\n",
    "    \n",
    "    # Read test data to then predict\n",
    "    df_test = pd.read_csv(input_path_test)\n",
    "\n",
    "    # Get first item of the dataset to get the shape of the input data\n",
    "    for element in data_reader(input_path_train):\n",
    "        input_shape = element[0].shape\n",
    "    \n",
    "    # Define and compile model\n",
    "    inputs = Input(shape=(input_shape[1],input_shape[2]), name=\"inputs\")\n",
    "    layer = Conv1D(250, kernel_size=7, strides=1, activation='relu', name=\"conv1\")(inputs)  # 250 7 relu\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Conv1D(250, 8, strides=1, activation='softmax', name=\"conv2\")(layer)  # 250 8 softmax\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = MaxPooling1D(pool_size=2, strides=None, name=\"maxpool1\")(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Conv1D(250, 3, strides=1, activation='softmax', name=\"conv3\")(layer)  # 250 3 softmax\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Conv1D(100, 2, strides=1, activation='softmax', name=\"conv4\")(layer)  # 100 3 softmax\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = MaxPooling1D(pool_size=1, strides=None, name=\"maxpool2\")(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(300, activation='sigmoid')(layer)  # 300\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Dense(200, activation='sigmoid')(layer)  # 300\n",
    "    predictions = Dense(1, activation='linear')(layer)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=norm_predictions)\n",
    "    model.summary()\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[\"mse\", \"mae\", \"mape\"],\n",
    "                )\n",
    "    # Run model\n",
    "    history=model.fit(data_reader(input_path_train, batch_size=200),\n",
    "                            epochs=20,\n",
    "                            validation_data=data_reader(input_path_valid,batch_size=100),\n",
    "                            callbacks=None,\n",
    "                            verbose=1)\n",
    "    \n",
    "    #After training we save the model weights to then run the contribution scores\n",
    "    model_path = '/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/Model_CV'+str(i)+\"_LibA_wide_pivot_state3.h5\"\n",
    "    model.save_weights(model_path, save_format='h5')\n",
    "    \n",
    "    # We predict the test data\n",
    "    predicted = model.predict(data_reader(input_path_test,\n",
    "                                                batch_size=100))\n",
    "\n",
    "    # We fill the dataframe with predictions and fold annotation\n",
    "    test_data = data_reader(input_path_test,batch_size=100)\n",
    "    test_tensor = X = np.empty(shape=[0,1])\n",
    "    for batch in test_data:\n",
    "        test_tensor = np.append(test_tensor, batch[1])\n",
    "\n",
    "    # Append fold to previous folds\n",
    "    df_test[\"prediction\"] = predicted\n",
    "    df_test[\"fold\"] = str(i)\n",
    "    df_test_10folds = pd.concat([df_test_10folds, df_test], ignore_index=True)    \n",
    "\n",
    "    # Append correlation coefficient and append to previous        \n",
    "    corr_coefficient = pearson_correlation(predicted.flatten(), test_tensor)\n",
    "    corr_list.append(corr_coefficient)\n",
    "\n",
    "df_test_10folds.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/LibA_wide_pivot_state3_test_predicted_cv10fold.csv\", index=False)\n",
    "print(corr_list)\n",
    "df_test_10folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "create_plots(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a0b4ec",
   "metadata": {},
   "source": [
    "# We now compute an ensemble approach we compute for each fold the model 10 times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48bb5d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "68/68 - 4s - loss: 0.1693 - mse: 0.1693 - mae: 0.2723 - mape: 29855.6074 - val_loss: 0.0208 - val_mse: 0.0208 - val_mae: 0.1004 - val_mape: 518.2103 - 4s/epoch - 59ms/step\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 11:04:24.773883: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 5652866676943124567\n",
      "2023-12-19 11:04:24.773949: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9647411216319893516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 - 1s - loss: 0.0221 - mse: 0.0221 - mae: 0.1154 - mape: 3656.4377 - val_loss: 0.0208 - val_mse: 0.0208 - val_mae: 0.1016 - val_mape: 566.4329 - 1s/epoch - 20ms/step\n",
      "Epoch 3/20\n",
      "68/68 - 1s - loss: 0.0184 - mse: 0.0184 - mae: 0.1054 - mape: 5359.4463 - val_loss: 0.0220 - val_mse: 0.0220 - val_mae: 0.1100 - val_mape: 816.8853 - 1s/epoch - 20ms/step\n",
      "Epoch 4/20\n",
      "68/68 - 1s - loss: 0.0148 - mse: 0.0148 - mae: 0.0936 - mape: 7677.7896 - val_loss: 0.0225 - val_mse: 0.0225 - val_mae: 0.1129 - val_mape: 880.3254 - 1s/epoch - 20ms/step\n",
      "Epoch 5/20\n",
      "68/68 - 1s - loss: 0.0126 - mse: 0.0126 - mae: 0.0858 - mape: 14431.0684 - val_loss: 0.0213 - val_mse: 0.0213 - val_mae: 0.1061 - val_mape: 717.3326 - 1s/epoch - 20ms/step\n",
      "Epoch 6/20\n",
      "68/68 - 1s - loss: 0.0106 - mse: 0.0106 - mae: 0.0790 - mape: 1411.5804 - val_loss: 0.0211 - val_mse: 0.0211 - val_mae: 0.1046 - val_mape: 677.1337 - 1s/epoch - 20ms/step\n",
      "Epoch 7/20\n",
      "68/68 - 1s - loss: 0.0097 - mse: 0.0097 - mae: 0.0762 - mape: 7867.0806 - val_loss: 0.0210 - val_mse: 0.0210 - val_mae: 0.1052 - val_mape: 709.4630 - 1s/epoch - 20ms/step\n",
      "Epoch 8/20\n",
      "68/68 - 1s - loss: 0.0088 - mse: 0.0088 - mae: 0.0724 - mape: 5609.1831 - val_loss: 0.0202 - val_mse: 0.0202 - val_mae: 0.0958 - val_mape: 325.4481 - 1s/epoch - 20ms/step\n",
      "Epoch 9/20\n",
      "68/68 - 1s - loss: 0.0079 - mse: 0.0079 - mae: 0.0686 - mape: 10299.8975 - val_loss: 0.0181 - val_mse: 0.0181 - val_mae: 0.0917 - val_mape: 247.5733 - 1s/epoch - 20ms/step\n",
      "Epoch 10/20\n",
      "68/68 - 1s - loss: 0.0074 - mse: 0.0074 - mae: 0.0669 - mape: 1042.9164 - val_loss: 0.0159 - val_mse: 0.0159 - val_mae: 0.0936 - val_mape: 656.9979 - 1s/epoch - 20ms/step\n",
      "Epoch 11/20\n",
      "68/68 - 1s - loss: 0.0071 - mse: 0.0071 - mae: 0.0659 - mape: 2390.6943 - val_loss: 0.0146 - val_mse: 0.0146 - val_mae: 0.0910 - val_mape: 587.9724 - 1s/epoch - 20ms/step\n",
      "Epoch 12/20\n",
      "68/68 - 1s - loss: 0.0076 - mse: 0.0076 - mae: 0.0682 - mape: 3176.0146 - val_loss: 0.0133 - val_mse: 0.0133 - val_mae: 0.0836 - val_mape: 220.9030 - 1s/epoch - 20ms/step\n",
      "Epoch 13/20\n",
      "68/68 - 1s - loss: 0.0087 - mse: 0.0087 - mae: 0.0741 - mape: 15928.8867 - val_loss: 0.0135 - val_mse: 0.0135 - val_mae: 0.0864 - val_mape: 409.8325 - 1s/epoch - 20ms/step\n",
      "Epoch 14/20\n",
      "68/68 - 1s - loss: 0.0107 - mse: 0.0107 - mae: 0.0821 - mape: 5940.1636 - val_loss: 0.0208 - val_mse: 0.0208 - val_mae: 0.1158 - val_mape: 933.4761 - 1s/epoch - 20ms/step\n",
      "Epoch 15/20\n",
      "68/68 - 1s - loss: 0.0116 - mse: 0.0116 - mae: 0.0863 - mape: 20876.9395 - val_loss: 0.0139 - val_mse: 0.0139 - val_mae: 0.0868 - val_mape: 447.8759 - 1s/epoch - 20ms/step\n",
      "Epoch 16/20\n",
      "68/68 - 1s - loss: 0.0081 - mse: 0.0081 - mae: 0.0710 - mape: 4186.9121 - val_loss: 0.0131 - val_mse: 0.0131 - val_mae: 0.0847 - val_mape: 376.5837 - 1s/epoch - 20ms/step\n",
      "Epoch 17/20\n",
      "68/68 - 1s - loss: 0.0072 - mse: 0.0072 - mae: 0.0672 - mape: 9327.6025 - val_loss: 0.0133 - val_mse: 0.0133 - val_mae: 0.0851 - val_mape: 416.8018 - 1s/epoch - 20ms/step\n",
      "Epoch 18/20\n",
      "68/68 - 1s - loss: 0.0076 - mse: 0.0076 - mae: 0.0690 - mape: 18890.9004 - val_loss: 0.0136 - val_mse: 0.0136 - val_mae: 0.0883 - val_mape: 464.8692 - 1s/epoch - 20ms/step\n",
      "Epoch 19/20\n",
      "68/68 - 1s - loss: 0.0080 - mse: 0.0080 - mae: 0.0714 - mape: 5663.9771 - val_loss: 0.0136 - val_mse: 0.0136 - val_mae: 0.0868 - val_mape: 329.0892 - 1s/epoch - 20ms/step\n",
      "Epoch 20/20\n",
      "68/68 - 1s - loss: 0.0112 - mse: 0.0112 - mae: 0.0847 - mape: 9591.7559 - val_loss: 0.0326 - val_mse: 0.0326 - val_mae: 0.1485 - val_mape: 1169.4447 - 1s/epoch - 20ms/step\n",
      "9/9 [==============================] - 0s 7ms/step\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/envs/tf_MPRA/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/felix/anaconda3/envs/tf_MPRA/lib/python3.9/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "df_test_10folds  = pd.DataFrame()\n",
    "corr_list = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    \n",
    "    input_path_train = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(i)+\"_LibA_wide_pivot_state3_train.csv\"\n",
    "    input_path_valid = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(i)+\"_LibA_wide_pivot_state3_validation.csv\"\n",
    "    input_path_test = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(i)+\"_LibA_wide_pivot_state3_test.csv\"\n",
    "   \n",
    "    df_test = pd.read_csv(input_path_test)\n",
    "    df_test[\"fold\"] = str(i)\n",
    "    corr_per_iteration = []\n",
    "    # Get first item of the dataset to get the shape of the input data\n",
    "    for element in data_reader(input_path_train):\n",
    "        input_shape = element[0].shape\n",
    "        \n",
    "    for iteration in range(1,11):\n",
    "        inputs = Input(shape=(input_shape[1],input_shape[2]), name=\"inputs\")\n",
    "        layer = Conv1D(250, kernel_size=7, strides=1, activation='relu', name=\"conv1\")(inputs)  # 250 7 relu\n",
    "        layer = Dropout(0.3)(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Conv1D(250, 8, strides=1, activation='softmax', name=\"conv2\")(layer)  # 250 8 softmax\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = MaxPooling1D(pool_size=2, strides=None, name=\"maxpool1\")(layer)\n",
    "        layer = Dropout(0.3)(layer)\n",
    "        layer = Conv1D(250, 3, strides=1, activation='softmax', name=\"conv3\")(layer)  # 250 3 softmax\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Dropout(0.3)(layer)\n",
    "        layer = Conv1D(100, 2, strides=1, activation='softmax', name=\"conv4\")(layer)  # 100 3 softmax\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = MaxPooling1D(pool_size=1, strides=None, name=\"maxpool2\")(layer)\n",
    "        layer = Dropout(0.3)(layer)\n",
    "        layer = Flatten()(layer)\n",
    "        layer = Dense(300, activation='sigmoid')(layer)  # 300\n",
    "        layer = Dropout(0.3)(layer)\n",
    "        layer = Dense(200, activation='sigmoid')(layer)  # 300\n",
    "        predictions = Dense(1, activation='linear')(layer)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        model.compile(optimizer=\"adam\",\n",
    "                    loss=\"mean_squared_error\",\n",
    "                    metrics=[\"mse\", \"mae\", \"mape\"],\n",
    "                    )\n",
    "\n",
    "        history=model.fit(data_reader(input_path_train, batch_size=100),\n",
    "                                epochs=20,\n",
    "                                validation_data=data_reader(input_path_valid,batch_size=100),\n",
    "                                callbacks=None,\n",
    "                                verbose=2)\n",
    "\n",
    "        predicted = model.predict(data_reader(input_path_test,\n",
    "                                                    batch_size=100))\n",
    "\n",
    "        test_data = data_reader(input_path_test,batch_size=100)\n",
    "        test_tensor = X = np.empty(shape=[0,1])\n",
    "        for batch in test_data:\n",
    "            test_tensor = np.append(test_tensor, batch[1])\n",
    "\n",
    "        df_test[\"prediction_iteration_\"+str(iteration)] = predicted\n",
    "        \n",
    "                \n",
    "        corr_coefficient = pearson_correlation(predicted.flatten(), test_tensor)\n",
    "        #corr_per_iteration.append(corr_coefficient)\n",
    "        break\n",
    "    \n",
    "    #df_test_10folds = df_test_10folds.append(df_test, ignore_index=True)\n",
    "    print(df_test_10folds)\n",
    "        \n",
    "    corr_ensemble = np.mean(corr_per_iteration)\n",
    "    corr_list.append(corr_ensemble)\n",
    "    break\n",
    "\n",
    "model.save_weights(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/simple_regression_cv1_test.h5\", save_format='h5') \n",
    "#df_test_10folds.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/LibA_wide_pivot_state3_test_predicted_cv10fold_ensemble.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
