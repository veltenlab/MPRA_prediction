{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7088f8",
   "metadata": {},
   "source": [
    "# Forward pass simple regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Input, Masking, Lambda, concatenate, Bidirectional, Dense, Dropout, Flatten, Conv1D,BatchNormalization,  MaxPooling1D, Bidirectional, GRU, TimeDistributed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fcc39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/test_background.h5\"\n",
    "\n",
    "# For the new data I am using training data just as an example\n",
    "new_data = \"\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/background_LibA_wide_pivot_state3_train_predicted_cv10fold.csv\"\"\n",
    "\n",
    "# Prepare input data for deep explainer\n",
    "df = pd.read_csv(new_data)\n",
    "\n",
    "def one_hot_encode_sequences(sequences, max_length, vocab):\n",
    "    \"\"\"One hot encodes a sequence of characters\n",
    "\n",
    "    Args:\n",
    "        sequences (np.array): Input sequences \n",
    "        vocab (list): list of vocabulary characters\n",
    "\n",
    "    Returns:\n",
    "        (np.array): one-hot-encoded array\n",
    "    \"\"\"\n",
    "    num_sequences = len(sequences)\n",
    "    max_seq_length = max_length\n",
    "    encoding = np.zeros((num_sequences, max_seq_length, len(vocab)), dtype=int)\n",
    "\n",
    "    # Create a dictionary to map characters to their corresponding indices in the vocab\n",
    "    char_to_index = {char: i for i, char in enumerate(vocab)}\n",
    "\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j, base in enumerate(sequence):\n",
    "            if base in char_to_index:\n",
    "                index = char_to_index[base]\n",
    "                encoding[i, j, index] = 1\n",
    "            else:\n",
    "                # Set the entire vector to [0, 0, 0, 0] for unexpected characters\n",
    "                encoding[i, j, :] = [0, 0, 0, 0]\n",
    "\n",
    "    return encoding\n",
    "\n",
    "vocab = [\"a\", \"c\", \"g\", \"t\"]\n",
    "max_length = df.seq.str.len().max()\n",
    "X = one_hot_encode_sequences(df.seq.values, max_length,vocab)\n",
    "y = df.State_3E.values\n",
    "ids = df.CRS.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Add trainable variables for mean and standard deviation\n",
    "        self.mean = self.add_weight(\"mean\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        self.stddev = self.add_weight(\"stddev\", shape=(1,), initializer=\"ones\", trainable=True)\n",
    "        super(CustomNormalization, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Normalize the inputs using the learned mean and standard deviation\n",
    "        return (inputs - self.mean) / (self.stddev + 1e-8)\n",
    "\n",
    "inputs = Input(shape=(X.shape[1], X.shape[2]), name=\"inputs\")\n",
    "#inputs = Masking()(inputs)\n",
    "layer = Conv1D(250, kernel_size=7, strides=1, activation='relu', name=\"conv1\")(inputs)  # 250 7 relu\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Conv1D(250, 8, strides=1, activation='softmax', name=\"conv2\")(layer)  # 250 8 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = MaxPooling1D(pool_size=2, strides=None, name=\"maxpool1\")(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Conv1D(250, 3, strides=1, activation='softmax', name=\"conv3\")(layer)  # 250 3 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Conv1D(100, 2, strides=1, activation='softmax', name=\"conv4\")(layer)  # 100 3 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = MaxPooling1D(pool_size=1, strides=None, name=\"maxpool2\")(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Flatten()(layer)\n",
    "layer = Dense(300, activation='sigmoid')(layer)  # 300\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Dense(200, activation='sigmoid')(layer)  # 300\n",
    "predictions = Dense(1, activation='linear')(layer)\n",
    "norm_predictions = CustomNormalization()(predictions)  # Assuming \"predictions\" is your existing output\n",
    "\n",
    "model = Model(inputs=inputs, outputs=norm_predictions)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=norm_predictions)\n",
    "model.summary()\n",
    "model.load_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dadcc3d",
   "metadata": {},
   "source": [
    "# Forward pass multihead regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8113abb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Add trainable variables for mean and standard deviation\n",
    "        self.mean = self.add_weight(\"mean\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        self.stddev = self.add_weight(\"stddev\", shape=(1,), initializer=\"ones\", trainable=True)\n",
    "        super(CustomNormalization, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Normalize the inputs using the learned mean and standard deviation\n",
    "        return (inputs - self.mean) / (self.stddev + 1e-8)\n",
    "\n",
    "inputs = Input(shape=(X.shape[1], X.shape[2]), name=\"inputs\")\n",
    "#inputs = Masking()(inputs)\n",
    "layer = Conv1D(250, kernel_size=7, strides=1, activation='relu', name=\"conv1\")(inputs)  # 250 7 relu\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Conv1D(250, 8, strides=1, activation='softmax', name=\"conv2\")(layer)  # 250 8 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = MaxPooling1D(pool_size=2, strides=None, name=\"maxpool1\")(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Conv1D(250, 3, strides=1, activation='softmax', name=\"conv3\")(layer)  # 250 3 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Conv1D(100, 2, strides=1, activation='softmax', name=\"conv4\")(layer)  # 100 3 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = MaxPooling1D(pool_size=1, strides=None, name=\"maxpool2\")(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Flatten()(layer)\n",
    "layer = Dense(300, activation='sigmoid')(layer)  # 300\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Dense(200, activation='sigmoid')(layer)  # 300\n",
    "predictions = Dense(1, activation='linear')(layer)\n",
    "norm_predictions = CustomNormalization()(predictions)  # Assuming \"predictions\" is your existing output\n",
    "\n",
    "model = Model(inputs=inputs, outputs=norm_predictions)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=norm_predictions)\n",
    "model.summary()\n",
    "model.load_weights(model_path)\n",
    "\n",
    "model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
