{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1f9e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-11 11:17:25.595511: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-11 11:17:25.627738: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-11 11:17:25.627781: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-11 11:17:25.627819: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-11 11:17:25.634684: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-11 11:17:27.188035: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21552 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:21:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Lambda, concatenate, Bidirectional, Dense, Dropout, Flatten, Conv1D,BatchNormalization,  MaxPooling1D, Bidirectional, GRU, TimeDistributed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "np.random.seed(1337) # for reproducibility\n",
    "vocab = [\"A\", \"G\", \"C\", \"T\"]\n",
    "indices = tf.range(len(vocab), dtype = tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab,indices)\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, 1)\n",
    "defs = [0.] * 1 + [tf.constant([], dtype = \"string\")]\n",
    "\n",
    "# Nadav dataset\n",
    "\n",
    "def data_reader(file, batch_size=100, n_parse_threads = 4):\n",
    "    dataset = tf.data.TextLineDataset(file).skip(1)\n",
    "    dataset=dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "def preprocess(record):\n",
    "    fields = tf.io.decode_csv(record, record_defaults=defs)\n",
    "    chars = tf.strings.bytes_split(fields[1])\n",
    "    chars_indeces = table.lookup(chars)\n",
    "    X = tf.one_hot(chars_indeces, depth = len(vocab))\n",
    "    Y = fields[0]\n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9eaa2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data in three partitions\n",
    "file=\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/LibA_wide_pivot_state3.csv\"\n",
    "whole_data = pd.read_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/LibA_wide_pivot_state3.csv\")\n",
    "\n",
    "df_train, df_test = train_test_split(whole_data, test_size=0.2, random_state=42)\n",
    "df_validation, df_test = train_test_split(df_test, test_size=0.5, random_state=42)\n",
    "\n",
    "df_train.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/LibA_wide_pivot_state3_train.csv\", index=False)\n",
    "df_test.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/LibA_wide_pivot_state3_test.csv\", index=False)\n",
    "df_validation.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/LibA_wide_pivot_state3_validation.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0250f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 262, 4)]          0         \n",
      "                                                                 \n",
      " conv1 (Conv1D)              (None, 256, 250)          7250      \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 256, 250)          0         \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 256, 250)          1000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " conv2 (Conv1D)              (None, 249, 250)          500250    \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 249, 250)          1000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " maxpool1 (MaxPooling1D)     (None, 124, 250)          0         \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 124, 250)          0         \n",
      "                                                                 \n",
      " conv3 (Conv1D)              (None, 122, 250)          187750    \n",
      "                                                                 \n",
      " batch_normalization_14 (Ba  (None, 122, 250)          1000      \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 122, 250)          0         \n",
      "                                                                 \n",
      " conv4 (Conv1D)              (None, 121, 100)          50100     \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 121, 100)          400       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " maxpool2 (MaxPooling1D)     (None, 121, 100)          0         \n",
      "                                                                 \n",
      " dropout_18 (Dropout)        (None, 121, 100)          0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 12100)             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 300)               3630300   \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        (None, 300)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 200)               60200     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4439451 (16.94 MB)\n",
      "Trainable params: 4437751 (16.93 MB)\n",
      "Non-trainable params: 1700 (6.64 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "68/68 [==============================] - 5s 32ms/step - loss: 0.0420 - mse: 0.0420 - mae: 0.1545 - mape: 26323.9355 - val_loss: 0.0221 - val_mse: 0.0221 - val_mae: 0.1000 - val_mape: 316.9471\n",
      "Epoch 2/20\n",
      "68/68 [==============================] - 2s 27ms/step - loss: 0.0178 - mse: 0.0178 - mae: 0.1016 - mape: 15944.6172 - val_loss: 0.0207 - val_mse: 0.0207 - val_mae: 0.0975 - val_mape: 1982.3201\n",
      "Epoch 3/20\n",
      "68/68 [==============================] - 2s 28ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0846 - mape: 2466.8337 - val_loss: 0.0203 - val_mse: 0.0203 - val_mae: 0.0970 - val_mape: 2874.8418\n",
      "Epoch 4/20\n",
      "68/68 [==============================] - 2s 27ms/step - loss: 0.0110 - mse: 0.0110 - mae: 0.0791 - mape: 5638.5645 - val_loss: 0.0213 - val_mse: 0.0213 - val_mae: 0.0985 - val_mape: 958.0336\n",
      "Epoch 5/20\n",
      "68/68 [==============================] - 2s 27ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0744 - mape: 1156.4105 - val_loss: 0.0208 - val_mse: 0.0208 - val_mae: 0.0976 - val_mape: 1809.8789\n",
      "Epoch 6/20\n",
      "68/68 [==============================] - 2s 28ms/step - loss: 0.0088 - mse: 0.0088 - mae: 0.0725 - mape: 6287.6260 - val_loss: 0.0202 - val_mse: 0.0202 - val_mae: 0.1049 - val_mape: 9716.1055\n",
      "Epoch 7/20\n",
      "68/68 [==============================] - 2s 28ms/step - loss: 0.0086 - mse: 0.0086 - mae: 0.0722 - mape: 10322.7578 - val_loss: 0.0191 - val_mse: 0.0191 - val_mae: 0.0974 - val_mape: 5975.0195\n",
      "Epoch 8/20\n",
      "68/68 [==============================] - 2s 28ms/step - loss: 0.0073 - mse: 0.0073 - mae: 0.0659 - mape: 10146.9629 - val_loss: 0.0180 - val_mse: 0.0180 - val_mae: 0.0983 - val_mape: 8097.4482\n",
      "Epoch 9/20\n",
      "68/68 [==============================] - 2s 29ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0622 - mape: 7159.2490 - val_loss: 0.0177 - val_mse: 0.0177 - val_mae: 0.1061 - val_mape: 12637.8896\n",
      "Epoch 10/20\n",
      "68/68 [==============================] - 2s 28ms/step - loss: 0.0066 - mse: 0.0066 - mae: 0.0630 - mape: 12070.6006 - val_loss: 0.0090 - val_mse: 0.0090 - val_mae: 0.0702 - val_mape: 2908.1685\n",
      "Epoch 11/20\n",
      "68/68 [==============================] - 2s 29ms/step - loss: 0.0067 - mse: 0.0067 - mae: 0.0637 - mape: 13543.3125 - val_loss: 0.0081 - val_mse: 0.0081 - val_mae: 0.0703 - val_mape: 7404.0234\n",
      "Epoch 12/20\n",
      "68/68 [==============================] - 2s 29ms/step - loss: 0.0085 - mse: 0.0085 - mae: 0.0735 - mape: 8597.4053 - val_loss: 0.0086 - val_mse: 0.0086 - val_mae: 0.0727 - val_mape: 4233.1475\n",
      "Epoch 13/20\n",
      "68/68 [==============================] - 2s 29ms/step - loss: 0.0077 - mse: 0.0077 - mae: 0.0691 - mape: 5323.0234 - val_loss: 0.0054 - val_mse: 0.0054 - val_mae: 0.0552 - val_mape: 2668.1494\n",
      "Epoch 14/20\n",
      "68/68 [==============================] - 2s 29ms/step - loss: 0.0065 - mse: 0.0065 - mae: 0.0633 - mape: 15320.4346 - val_loss: 0.0072 - val_mse: 0.0072 - val_mae: 0.0673 - val_mape: 6900.9424\n",
      "Epoch 15/20\n",
      "68/68 [==============================] - 2s 28ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0612 - mape: 18564.0664 - val_loss: 0.0093 - val_mse: 0.0093 - val_mae: 0.0791 - val_mape: 8988.7764\n",
      "Epoch 16/20\n",
      "68/68 [==============================] - 2s 29ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0594 - mape: 4456.5947 - val_loss: 0.0053 - val_mse: 0.0053 - val_mae: 0.0552 - val_mape: 2850.8909\n",
      "Epoch 17/20\n",
      "68/68 [==============================] - 2s 29ms/step - loss: 0.0055 - mse: 0.0055 - mae: 0.0579 - mape: 8312.0342 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0430 - val_mape: 3405.4463\n",
      "Epoch 18/20\n",
      "68/68 [==============================] - 2s 29ms/step - loss: 0.0051 - mse: 0.0051 - mae: 0.0557 - mape: 7225.9209 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0430 - val_mape: 1369.2311\n",
      "Epoch 19/20\n",
      "68/68 [==============================] - 2s 28ms/step - loss: 0.0049 - mse: 0.0049 - mae: 0.0553 - mape: 7124.3706 - val_loss: 0.0037 - val_mse: 0.0037 - val_mae: 0.0437 - val_mape: 874.6454\n",
      "Epoch 20/20\n",
      "68/68 [==============================] - 2s 28ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0600 - mape: 12044.8838 - val_loss: 0.0043 - val_mse: 0.0043 - val_mae: 0.0482 - val_mape: 5601.3770\n",
      "9/9 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "input_path_train = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/LibA_wide_pivot_state3_train.csv\"\n",
    "input_path_valid = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/LibA_wide_pivot_state3_validation.csv\"\n",
    "input_path_test = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/LibA_wide_pivot_state3_test.csv\"\n",
    "\n",
    "corr_list = []\n",
    "df_repetition = pd.DataFrame()\n",
    "\n",
    "for i in range(0,10):\n",
    "    # Get first item of the dataset to get the shape of the input data\n",
    "    for element in data_reader(file):\n",
    "        input_shape = element[0].shape\n",
    "\n",
    "    inputs = Input(shape=(input_shape[1],input_shape[2]), name=\"inputs\")\n",
    "    layer = Conv1D(250, kernel_size=7, strides=1, activation='relu', name=\"conv1\")(inputs)  # 250 7 relu\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Conv1D(250, 8, strides=1, activation='softmax', name=\"conv2\")(layer)  # 250 8 softmax\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = MaxPooling1D(pool_size=2, strides=None, name=\"maxpool1\")(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Conv1D(250, 3, strides=1, activation='softmax', name=\"conv3\")(layer)  # 250 3 softmax\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Conv1D(100, 2, strides=1, activation='softmax', name=\"conv4\")(layer)  # 100 3 softmax\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = MaxPooling1D(pool_size=1, strides=None, name=\"maxpool2\")(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(300, activation='sigmoid')(layer)  # 300\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Dense(200, activation='sigmoid')(layer)  # 300\n",
    "    predictions = Dense(1, activation='linear')(layer)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[\"mse\", \"mae\", \"mape\"],\n",
    "                )\n",
    "\n",
    "    history=model.fit(data_reader(input_path_train, batch_size=100),\n",
    "                            epochs=20,\n",
    "                            validation_data=data_reader(file,batch_size=100),\n",
    "                            callbacks=None,\n",
    "                            verbose=1)\n",
    "\n",
    "    predicted = model.predict(data_reader(input_path_test,\n",
    "                                                batch_size=100))\n",
    "\n",
    "    test_data = data_reader(input_path_test,batch_size=100)\n",
    "    test_tensor = X = np.empty(shape=[0,1])\n",
    "    for batch in test_data:\n",
    "        test_tensor = np.append(test_tensor, batch[1])\n",
    "\n",
    "    import math\n",
    "    def pearson_correlation(x, y):\n",
    "        n = len(x)\n",
    "        # Calculate the mean of x and y\n",
    "        mean_x = sum(x) / n\n",
    "        mean_y = sum(y) / n\n",
    "        \n",
    "        # Calculate the numerator and denominators of the correlation coefficient\n",
    "        numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n",
    "        denominator_x = math.sqrt(sum((xi - mean_x) ** 2 for xi in x))\n",
    "        denominator_y = math.sqrt(sum((yi - mean_y) ** 2 for yi in y))\n",
    "        \n",
    "        # Calculate the correlation coefficient\n",
    "        correlation = numerator / (denominator_x * denominator_y)\n",
    "        return correlation\n",
    "        \n",
    "    corr_coefficient = pearson_correlation(predicted.flatten(), test_tensor)\n",
    "    corr_list.append(corr_coefficient)\n",
    "    df_repetition[str(i)] =predicted.flatten()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/Model_CV1_LibA_wide_pivot_state3.h5\", save_format='h5') \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
