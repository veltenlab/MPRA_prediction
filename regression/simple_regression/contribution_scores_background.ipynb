{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f7088f8",
   "metadata": {},
   "source": [
    "# MPRA interpretation using SHAP values to obtain contribution scores on background samples\n",
    "\n",
    "### Environment\n",
    "The next chunk contains the commands necessary to install the environment required to run this jupyter notebook\n",
    "Skip this chunk if the installation was previously done. The github repo contains a yml file with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1f9e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Layer, Dense, Dropout, Flatten, Conv1D,BatchNormalization,  MaxPooling1D\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db929c",
   "metadata": {},
   "source": [
    "### One-hot-encoding\n",
    "\n",
    "We define the method for one-hot-encoding, here we are not using the dataset class so we define the one-hot-encoder method outside of the dataset tensorflow framework\n",
    "In this case the vocabulary is different compared to the normal contribution scores (we encode where there is no motif; lower cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89f9e7ca",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f5f51e60d8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"g\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"t\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_encode_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mState_3E\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCRS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-f5f51e60d8e8>\u001b[0m in \u001b[0;36mone_hot_encode_sequences\u001b[0;34m(sequences, max_length, vocab)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchar_to_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar_to_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m                 \u001b[0mencoding\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;31m# Set the entire vector to [0, 0, 0, 0] for unexpected characters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Prepare input data for deep explainer\n",
    "df = pd.read_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/background_LibA_wide_pivot_state3_test_predicted_cv10fold.csv\")\n",
    "\n",
    "def one_hot_encode_sequences(sequences, max_length, vocab):\n",
    "    \"\"\"One hot encodes a sequence of characters\n",
    "\n",
    "    Args:\n",
    "        sequences (np.array): Input sequences \n",
    "        vocab (list): list of vocabulary characters\n",
    "\n",
    "    Returns:\n",
    "        (np.array): one-hot-encoded array\n",
    "    \"\"\"\n",
    "    num_sequences = len(sequences)\n",
    "    max_seq_length = max_length\n",
    "    encoding = np.zeros((num_sequences, max_seq_length, len(vocab)), dtype=int)\n",
    "\n",
    "    # Create a dictionary to map characters to their corresponding indices in the vocab\n",
    "    char_to_index = {char: i for i, char in enumerate(vocab)}\n",
    "\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j, base in enumerate(sequence):\n",
    "            if base in char_to_index:\n",
    "                index = char_to_index[base]\n",
    "                encoding[i, j, index] = 1\n",
    "            else:\n",
    "                # Set the entire vector to [0, 0, 0, 0] for unexpected characters\n",
    "                encoding[i, j, :] = [0, 0, 0, 0]\n",
    "\n",
    "    return encoding\n",
    "\n",
    "vocab = [\"a\", \"c\", \"g\", \"t\"]\n",
    "max_length = df.seq.str.len().max()\n",
    "X = one_hot_encode_sequences(df.seq.values, max_length,vocab)\n",
    "y = df.State_3E.values\n",
    "ids = df.CRS.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc0eaf",
   "metadata": {},
   "source": [
    "### Model definition and loading of trained weights\n",
    "Since for DeepExplainer we need an older tensorflow version we define the model and then load the weights (we can't load the model entirely since it comes from a different tensorflow version )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f0140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method CustomNormalization.call of <__main__.CustomNormalization object at 0x7fa7b5a6b9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Name constructor takes either 0 or 4 mandatory arguments\n",
      "WARNING: Entity <bound method CustomNormalization.call of <__main__.CustomNormalization object at 0x7fa7b5a6b9b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Name constructor takes either 0 or 4 mandatory arguments\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inputs (InputLayer)          [(None, 262, 4)]          0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv1D)               (None, 256, 250)          7250      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 256, 250)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256, 250)          1000      \n",
      "_________________________________________________________________\n",
      "conv2 (Conv1D)               (None, 249, 250)          500250    \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 249, 250)          1000      \n",
      "_________________________________________________________________\n",
      "maxpool1 (MaxPooling1D)      (None, 124, 250)          0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 124, 250)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv1D)               (None, 122, 250)          187750    \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 122, 250)          1000      \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 122, 250)          0         \n",
      "_________________________________________________________________\n",
      "conv4 (Conv1D)               (None, 121, 100)          50100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 121, 100)          400       \n",
      "_________________________________________________________________\n",
      "maxpool2 (MaxPooling1D)      (None, 121, 100)          0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 121, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12100)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               3630300   \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 200)               60200     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 201       \n",
      "_________________________________________________________________\n",
      "custom_normalization_1 (Cust (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 4,439,453\n",
      "Trainable params: 4,437,753\n",
      "Non-trainable params: 1,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class CustomNormalization(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Add trainable variables for mean and standard deviation\n",
    "        self.mean = self.add_weight(\"mean\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        self.stddev = self.add_weight(\"stddev\", shape=(1,), initializer=\"ones\", trainable=True)\n",
    "        super(CustomNormalization, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Normalize the inputs using the learned mean and standard deviation\n",
    "        return (inputs - self.mean) / (self.stddev + 1e-8)\n",
    "\n",
    "inputs = Input(shape=(X.shape[1], X.shape[2]), name=\"inputs\")\n",
    "#inputs = Masking()(inputs)\n",
    "layer = Conv1D(250, kernel_size=7, strides=1, activation='relu', name=\"conv1\")(inputs)  # 250 7 relu\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Conv1D(250, 8, strides=1, activation='softmax', name=\"conv2\")(layer)  # 250 8 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = MaxPooling1D(pool_size=2, strides=None, name=\"maxpool1\")(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Conv1D(250, 3, strides=1, activation='softmax', name=\"conv3\")(layer)  # 250 3 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Conv1D(100, 2, strides=1, activation='softmax', name=\"conv4\")(layer)  # 100 3 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = MaxPooling1D(pool_size=1, strides=None, name=\"maxpool2\")(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Flatten()(layer)\n",
    "layer = Dense(300, activation='sigmoid')(layer)  # 300\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Dense(200, activation='sigmoid')(layer)  # 300\n",
    "predictions = Dense(1, activation='linear')(layer)\n",
    "norm_predictions = CustomNormalization()(predictions)  # Assuming \"predictions\" is your existing output\n",
    "\n",
    "model = Model(inputs=inputs, outputs=norm_predictions)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=norm_predictions)\n",
    "model.summary()\n",
    "model.load_weights(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/test_background.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d1670f",
   "metadata": {},
   "source": [
    "### Plotting methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f8e837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "def plot_a(ax, base, left_edge, height, color):\n",
    "    a_polygon_coords = [\n",
    "        np.array([\n",
    "           [0.0, 0.0],\n",
    "           [0.5, 1.0],\n",
    "           [0.5, 0.8],\n",
    "           [0.2, 0.0],\n",
    "        ]),\n",
    "        np.array([\n",
    "           [1.0, 0.0],\n",
    "           [0.5, 1.0],\n",
    "           [0.5, 0.8],\n",
    "           [0.8, 0.0],\n",
    "        ]),\n",
    "        np.array([\n",
    "           [0.225, 0.45],\n",
    "           [0.775, 0.45],\n",
    "           [0.85, 0.3],\n",
    "           [0.15, 0.3],\n",
    "        ])\n",
    "    ]\n",
    "    for polygon_coords in a_polygon_coords:\n",
    "        ax.add_patch(matplotlib.patches.Polygon((np.array([1,height])[None,:]*polygon_coords\n",
    "                                                 + np.array([left_edge,base])[None,:]),\n",
    "                                                facecolor=color, edgecolor=color))\n",
    "\n",
    "def plot_c(ax, base, left_edge, height, color):\n",
    "    ax.add_patch(matplotlib.patches.Ellipse(xy=[left_edge+0.65, base+0.5*height], width=1.3, height=height,\n",
    "                                            facecolor=color, edgecolor=color))\n",
    "    ax.add_patch(matplotlib.patches.Ellipse(xy=[left_edge+0.65, base+0.5*height], width=0.7*1.3, height=0.7*height,\n",
    "                                            facecolor='white', edgecolor='white'))\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge+1, base], width=1.0, height=height,\n",
    "                                            facecolor='white', edgecolor='white', fill=True))\n",
    "\n",
    "def plot_g(ax, base, left_edge, height, color):\n",
    "    ax.add_patch(matplotlib.patches.Ellipse(xy=[left_edge+0.65, base+0.5*height], width=1.3, height=height,\n",
    "                                            facecolor=color, edgecolor=color))\n",
    "    ax.add_patch(matplotlib.patches.Ellipse(xy=[left_edge+0.65, base+0.5*height], width=0.7*1.3, height=0.7*height,\n",
    "                                            facecolor='white', edgecolor='white'))\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge+1, base], width=1.0, height=height,\n",
    "                                            facecolor='white', edgecolor='white', fill=True))\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge+0.825, base+0.085*height], width=0.174, height=0.415*height,\n",
    "                                            facecolor=color, edgecolor=color, fill=True))\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge+0.625, base+0.35*height], width=0.374, height=0.15*height,\n",
    "                                            facecolor=color, edgecolor=color, fill=True))\n",
    "\n",
    "def plot_t(ax, base, left_edge, height, color):\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge+0.4, base],\n",
    "                  width=0.2, height=height, facecolor=color, edgecolor=color, fill=True))\n",
    "    ax.add_patch(matplotlib.patches.Rectangle(xy=[left_edge, base+0.8*height],\n",
    "                  width=1.0, height=0.2*height, facecolor=color, edgecolor=color, fill=True))\n",
    "\n",
    "default_colors = {0:'green', 1:'blue', 2:'orange', 3:'red'}\n",
    "default_plot_funcs = {0:plot_a, 1:plot_c, 2:plot_g, 3:plot_t}\n",
    "\n",
    "default_colors = {0:'green', 1:'blue', 2:'orange', 3:'red'}\n",
    "default_plot_funcs = {0:plot_a, 1:plot_c, 2:plot_g, 3:plot_t}\n",
    "\n",
    "def plot_weights_given_ax(ax, array,\n",
    "                 height_padding_factor,\n",
    "                 length_padding,\n",
    "                 subticks_frequency,\n",
    "                 highlight,\n",
    "                 colors=default_colors,\n",
    "                 plot_funcs=default_plot_funcs):\n",
    "    if len(array.shape)==3:\n",
    "        array = np.squeeze(array)\n",
    "    assert len(array.shape)==2, array.shape\n",
    "    if (array.shape[0]==4 and array.shape[1] != 4):\n",
    "        array = array.transpose(1,0)\n",
    "    assert array.shape[1]==4\n",
    "    max_pos_height = 0.0\n",
    "    min_neg_height = 0.0\n",
    "    heights_at_positions = []\n",
    "    depths_at_positions = []\n",
    "    for i in range(array.shape[0]):\n",
    "        acgt_vals = sorted(enumerate(array[i,:]), key=lambda x: abs(x[1]))\n",
    "        positive_height_so_far = 0.0\n",
    "        negative_height_so_far = 0.0\n",
    "        for letter in acgt_vals:\n",
    "            plot_func = plot_funcs[letter[0]]\n",
    "            color=colors[letter[0]]\n",
    "            if (letter[1] > 0):\n",
    "                height_so_far = positive_height_so_far\n",
    "                positive_height_so_far += letter[1]                \n",
    "            else:\n",
    "                height_so_far = negative_height_so_far\n",
    "                negative_height_so_far += letter[1]\n",
    "            plot_func(ax=ax, base=height_so_far, left_edge=i, height=letter[1], color=color)\n",
    "        max_pos_height = max(max_pos_height, positive_height_so_far)\n",
    "        min_neg_height = min(min_neg_height, negative_height_so_far)\n",
    "        heights_at_positions.append(positive_height_so_far)\n",
    "        depths_at_positions.append(negative_height_so_far)\n",
    "    for color in highlight:\n",
    "        for start_pos, end_pos in highlight[color]:\n",
    "            assert start_pos >= 0.0 and end_pos <= array.shape[0]\n",
    "            min_depth = np.min(depths_at_positions[start_pos:end_pos])\n",
    "            max_height = np.max(heights_at_positions[start_pos:end_pos])\n",
    "            ax.add_patch(\n",
    "                matplotlib.patches.Rectangle(xy=[start_pos,min_depth],\n",
    "                    width=end_pos-start_pos,\n",
    "                    height=max_height-min_depth,\n",
    "                    edgecolor=color, fill=False))\n",
    "            \n",
    "    ax.set_xlim(-length_padding, array.shape[0]+length_padding)\n",
    "    ax.xaxis.set_ticks(np.arange(0.0, array.shape[0]+1, subticks_frequency))\n",
    "    height_padding = max(abs(min_neg_height)*(height_padding_factor),\n",
    "                         abs(max_pos_height)*(height_padding_factor))\n",
    "    ax.set_ylim(min_neg_height-height_padding, max_pos_height+height_padding)\n",
    "    return ax\n",
    "\n",
    "def plot_weights_modified(array, fig, n,n1,n2, title='', ylab='',\n",
    "                              figsize=(20,2),\n",
    "                 height_padding_factor=0.2,\n",
    "                 length_padding=1.0,\n",
    "                 subticks_frequency=20,\n",
    "                 colors=default_colors,\n",
    "                 plot_funcs=default_plot_funcs,\n",
    "                 highlight={}):\n",
    "    ax = fig.add_subplot(n,n1,n2) \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel(ylab)\n",
    "    y = plot_weights_given_ax(ax=ax, array=array,\n",
    "        height_padding_factor=height_padding_factor,\n",
    "        length_padding=length_padding,\n",
    "        subticks_frequency=subticks_frequency,\n",
    "        colors=colors,\n",
    "        plot_funcs=plot_funcs,\n",
    "        highlight=highlight)\n",
    "    return fig,ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb35a5a",
   "metadata": {},
   "source": [
    "### Initialize Deep Explainer\n",
    "\n",
    "Deep explainer is an approximation of shap values, the output prediction is decomposed by backpropagating the contributions of all networks to every input feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "rn=np.random.choice([X,  X[:,::-1,::-1]][0].shape[0], 1000, replace=False)\n",
    "explainer = shap.DeepExplainer((model.inputs, model.layers[-1].output), X[rn])\n",
    "\n",
    "if os.path.exists(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/deep_explainer_background/\") is False:\n",
    "    os.mkdir(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/deep_explainer_background/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f29633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "shap_values, indexes = explainer.shap_values(X, ranked_outputs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ffc46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "[array([[[ 0.,  0.,  0.,  0.],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        ...,\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]],\n",
       "\n",
       "       [[nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan]],\n",
       "\n",
       "       [[nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        ...,\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan],\n",
       "        [nan, nan, nan, nan]]])]"
      ],
      "text/plain": [
       "[array([[[ 0.,  0.,  0.,  0.],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.],\n",
       "         [ 0.,  0.,  0.,  0.]],\n",
       " \n",
       "        [[nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan]],\n",
       " \n",
       "        [[nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan]],\n",
       " \n",
       "        [[nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan]],\n",
       " \n",
       "        [[nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         ...,\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan],\n",
       "         [nan, nan, nan, nan]]])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbcb9b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each observation, we plot the importance scores\n",
    "for i in range(0, X.shape[0]):\n",
    "    ntrack=2\n",
    "    fig = plt.figure(figsize=(80,10))\n",
    "    _, ax1 =plot_weights_modified(shap_values[0][0]*X[i],fig,ntrack,1,1,title=i, subticks_frequency=10,ylab=\"DeepExplainer\")\n",
    "    break\n",
    "    plt.savefig(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/deep_explainer_background/\"+ids[i] +'_deep_explainer.png')\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd259b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shap_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1521a24f4fd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mshap_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'shap_values' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffa280e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
