{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Lambda, concatenate, Bidirectional, Dense, Dropout, Flatten, Conv1D,BatchNormalization,  MaxPooling1D, Bidirectional, GRU, TimeDistributed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "np.random.seed(1337) # for reproducibility\n",
    "vocab = [\"A\", \"G\", \"C\", \"T\"]\n",
    "indices = tf.range(len(vocab), dtype = tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab,indices)\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, 1)\n",
    "defs = [0.] * 1 + [tf.constant([], dtype = \"string\")]\n",
    "\n",
    "# Nadav dataset\n",
    "\n",
    "def data_reader(file, batch_size=100, n_parse_threads = 4):\n",
    "    dataset = tf.data.TextLineDataset(file).skip(1)\n",
    "    dataset=dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "def preprocess(record):\n",
    "    fields = tf.io.decode_csv(record, record_defaults=defs)\n",
    "    chars = tf.strings.bytes_split(fields[1])\n",
    "    chars_indeces = table.lookup(chars)\n",
    "    X = tf.one_hot(chars_indeces, depth = len(vocab))\n",
    "    Y = fields[0]\n",
    "    return X,Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6395a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS VALIDATION (10 fold)\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "# Split the data in three partitions\n",
    "whole_data = pd.read_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/LibA_wide_pivot_state3.csv\")\n",
    "\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 2008)\n",
    "\n",
    "o=1\n",
    "for i in kf.split(whole_data):\n",
    "    train = whole_data.iloc[i[0]]\n",
    "    test =  whole_data.iloc[i[1]]\n",
    "    \n",
    "    train, validation = train_test_split(train, test_size=0.10, random_state=42)\n",
    "    \n",
    "    train.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(o)+\"_LibA_wide_pivot_state3_train.csv\", index=False)\n",
    "    test.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(o)+\"_LibA_wide_pivot_state3_test.csv\", index=False)\n",
    "    validation.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(o)+\"_LibA_wide_pivot_state3_validation.csv\", index=False)\n",
    "    o+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03d1457f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [State_3E, seq, prediction]\n",
      "Index: []\n",
      "Model: \"model_42\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 262, 4)]          0         \n",
      "                                                                 \n",
      " conv1 (Conv1D)              (None, 256, 250)          7250      \n",
      "                                                                 \n",
      " dropout_210 (Dropout)       (None, 256, 250)          0         \n",
      "                                                                 \n",
      " batch_normalization_168 (B  (None, 256, 250)          1000      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " conv2 (Conv1D)              (None, 249, 250)          500250    \n",
      "                                                                 \n",
      " batch_normalization_169 (B  (None, 249, 250)          1000      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " maxpool1 (MaxPooling1D)     (None, 124, 250)          0         \n",
      "                                                                 \n",
      " dropout_211 (Dropout)       (None, 124, 250)          0         \n",
      "                                                                 \n",
      " conv3 (Conv1D)              (None, 122, 250)          187750    \n",
      "                                                                 \n",
      " batch_normalization_170 (B  (None, 122, 250)          1000      \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " dropout_212 (Dropout)       (None, 122, 250)          0         \n",
      "                                                                 \n",
      " conv4 (Conv1D)              (None, 121, 100)          50100     \n",
      "                                                                 \n",
      " batch_normalization_171 (B  (None, 121, 100)          400       \n",
      " atchNormalization)                                              \n",
      "                                                                 \n",
      " maxpool2 (MaxPooling1D)     (None, 121, 100)          0         \n",
      "                                                                 \n",
      " dropout_213 (Dropout)       (None, 121, 100)          0         \n",
      "                                                                 \n",
      " flatten_42 (Flatten)        (None, 12100)             0         \n",
      "                                                                 \n",
      " dense_126 (Dense)           (None, 300)               3630300   \n",
      "                                                                 \n",
      " dropout_214 (Dropout)       (None, 300)               0         \n",
      "                                                                 \n",
      " dense_127 (Dense)           (None, 200)               60200     \n",
      "                                                                 \n",
      " dense_128 (Dense)           (None, 1)                 201       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4439451 (16.94 MB)\n",
      "Trainable params: 4437751 (16.93 MB)\n",
      "Non-trainable params: 1700 (6.64 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "     33/Unknown - 4s 29ms/step - loss: 0.0548 - mse: 0.0548 - mae: 0.1825 - mape: 28851.5293"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 16:43:10.443960: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 3098329268809778405\n",
      "2023-10-17 16:43:10.444046: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15565383368236152796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 5s 39ms/step - loss: 0.0536 - mse: 0.0536 - mae: 0.1804 - mape: 27755.7539 - val_loss: 0.0246 - val_mse: 0.0246 - val_mae: 0.1222 - val_mape: 1118.2972\n",
      "Epoch 2/20\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0241 - mse: 0.0241 - mae: 0.1207 - mape: 3645.5276 - val_loss: 0.0223 - val_mse: 0.0223 - val_mae: 0.0997 - val_mape: 200.6152\n",
      "Epoch 3/20\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0178 - mse: 0.0178 - mae: 0.1029 - mape: 11250.7090 - val_loss: 0.0225 - val_mse: 0.0225 - val_mae: 0.1000 - val_mape: 179.1040\n",
      "Epoch 4/20\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0127 - mse: 0.0127 - mae: 0.0864 - mape: 2802.2793 - val_loss: 0.0212 - val_mse: 0.0212 - val_mae: 0.0987 - val_mape: 354.5633\n",
      "Epoch 5/20\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0105 - mse: 0.0105 - mae: 0.0784 - mape: 7850.2495 - val_loss: 0.0211 - val_mse: 0.0211 - val_mae: 0.0988 - val_mape: 395.8640\n",
      "Epoch 6/20\n",
      "35/35 [==============================] - 1s 35ms/step - loss: 0.0095 - mse: 0.0095 - mae: 0.0746 - mape: 6026.8687 - val_loss: 0.0222 - val_mse: 0.0222 - val_mae: 0.0994 - val_mape: 216.3631\n",
      "Epoch 7/20\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0101 - mse: 0.0101 - mae: 0.0788 - mape: 3121.0098 - val_loss: 0.0217 - val_mse: 0.0217 - val_mae: 0.1083 - val_mape: 820.2236\n",
      "Epoch 8/20\n",
      "35/35 [==============================] - 1s 33ms/step - loss: 0.0101 - mse: 0.0101 - mae: 0.0785 - mape: 2412.3303 - val_loss: 0.0253 - val_mse: 0.0253 - val_mae: 0.1252 - val_mape: 1169.7261\n",
      "Epoch 9/20\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0071 - mse: 0.0071 - mae: 0.0650 - mape: 11375.5908 - val_loss: 0.0222 - val_mse: 0.0222 - val_mae: 0.1109 - val_mape: 886.0153\n",
      "Epoch 10/20\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0058 - mse: 0.0058 - mae: 0.0591 - mape: 4883.0718 - val_loss: 0.0211 - val_mse: 0.0211 - val_mae: 0.1039 - val_mape: 682.0875\n",
      "Epoch 11/20\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0056 - mse: 0.0056 - mae: 0.0581 - mape: 933.1824 - val_loss: 0.0209 - val_mse: 0.0209 - val_mae: 0.1024 - val_mape: 623.9999\n",
      "Epoch 12/20\n",
      "35/35 [==============================] - 1s 35ms/step - loss: 0.0052 - mse: 0.0052 - mae: 0.0555 - mape: 10034.5342 - val_loss: 0.0207 - val_mse: 0.0207 - val_mae: 0.0997 - val_mape: 495.5373\n",
      "Epoch 13/20\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0048 - mse: 0.0048 - mae: 0.0538 - mape: 8227.5127 - val_loss: 0.0207 - val_mse: 0.0207 - val_mae: 0.1017 - val_mape: 611.0210\n",
      "Epoch 14/20\n",
      "35/35 [==============================] - 1s 35ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0553 - mape: 2612.7490 - val_loss: 0.0211 - val_mse: 0.0211 - val_mae: 0.1063 - val_mape: 788.0316\n",
      "Epoch 15/20\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0054 - mse: 0.0054 - mae: 0.0570 - mape: 1907.8342 - val_loss: 0.0248 - val_mse: 0.0248 - val_mae: 0.1252 - val_mape: 1176.4724\n",
      "Epoch 16/20\n",
      "35/35 [==============================] - 1s 35ms/step - loss: 0.0061 - mse: 0.0061 - mae: 0.0611 - mape: 2880.1248 - val_loss: 0.0194 - val_mse: 0.0194 - val_mae: 0.0945 - val_mape: 331.6071\n",
      "Epoch 17/20\n",
      "35/35 [==============================] - 1s 35ms/step - loss: 0.0057 - mse: 0.0057 - mae: 0.0591 - mape: 11061.0918 - val_loss: 0.0285 - val_mse: 0.0285 - val_mae: 0.1222 - val_mape: 645.4049\n",
      "Epoch 18/20\n",
      "35/35 [==============================] - 1s 35ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0551 - mape: 5054.2622 - val_loss: 0.0211 - val_mse: 0.0211 - val_mae: 0.1028 - val_mape: 454.6419\n",
      "Epoch 19/20\n",
      "35/35 [==============================] - 1s 34ms/step - loss: 0.0050 - mse: 0.0050 - mae: 0.0553 - mape: 11204.2334 - val_loss: 0.0171 - val_mse: 0.0171 - val_mae: 0.0906 - val_mape: 260.5987\n",
      "Epoch 20/20\n",
      "35/35 [==============================] - 1s 35ms/step - loss: 0.0046 - mse: 0.0046 - mae: 0.0533 - mape: 2217.9587 - val_loss: 0.0203 - val_mse: 0.0203 - val_mae: 0.1032 - val_mape: 493.5666\n",
      "9/9 [==============================] - 0s 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-17 16:43:35.285771: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 15565383368236152796\n",
      "/tmp/ipykernel_738500/2195724150.py:62: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df_test_overall = df_test_overall.append(df_test, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "df_test_overall  = pd.DataFrame(columns=['State_3E', \"seq\", \"prediction\"])\n",
    "print(df_test_overall)\n",
    "corr_list = []\n",
    "\n",
    "for i in range(1,2):\n",
    "    \n",
    "    input_path_train = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(i)+\"_LibA_wide_pivot_state3_train.csv\"\n",
    "    input_path_valid = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(i)+\"_LibA_wide_pivot_state3_validation.csv\"\n",
    "    input_path_test = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/CV\"+str(i)+\"_LibA_wide_pivot_state3_test.csv\"\n",
    "    \n",
    "    df_test = pd.read_csv(input_path_test)\n",
    "\n",
    "    # Get first item of the dataset to get the shape of the input data\n",
    "    for element in data_reader(input_path_train):\n",
    "        input_shape = element[0].shape\n",
    "\n",
    "    inputs = Input(shape=(input_shape[1],input_shape[2]), name=\"inputs\")\n",
    "    layer = Conv1D(250, kernel_size=7, strides=1, activation='relu', name=\"conv1\")(inputs)  # 250 7 relu\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Conv1D(250, 8, strides=1, activation='softmax', name=\"conv2\")(layer)  # 250 8 softmax\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = MaxPooling1D(pool_size=2, strides=None, name=\"maxpool1\")(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Conv1D(250, 3, strides=1, activation='softmax', name=\"conv3\")(layer)  # 250 3 softmax\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Conv1D(100, 2, strides=1, activation='softmax', name=\"conv4\")(layer)  # 100 3 softmax\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = MaxPooling1D(pool_size=1, strides=None, name=\"maxpool2\")(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Flatten()(layer)\n",
    "    layer = Dense(300, activation='sigmoid')(layer)  # 300\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Dense(200, activation='sigmoid')(layer)  # 300\n",
    "    predictions = Dense(1, activation='linear')(layer)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=predictions)\n",
    "    model.summary()\n",
    "\n",
    "    model.compile(optimizer=\"adam\",\n",
    "                loss=\"mean_squared_error\",\n",
    "                metrics=[\"mse\", \"mae\", \"mape\"],\n",
    "                )\n",
    "\n",
    "    history=model.fit(data_reader(input_path_train, batch_size=200),\n",
    "                            epochs=20,\n",
    "                            validation_data=data_reader(input_path_valid,batch_size=100),\n",
    "                            callbacks=None,\n",
    "                            verbose=1)\n",
    "\n",
    "    predicted = model.predict(data_reader(input_path_test,\n",
    "                                                batch_size=100))\n",
    "\n",
    "    test_data = data_reader(input_path_test,batch_size=100)\n",
    "    test_tensor = X = np.empty(shape=[0,1])\n",
    "    for batch in test_data:\n",
    "        test_tensor = np.append(test_tensor, batch[1])\n",
    "\n",
    "    df_test[\"prediction\"] = predicted\n",
    "    df_test[\"fold\"] = str(i)\n",
    "    df_test_overall = df_test_overall.append(df_test, ignore_index=True)\n",
    "    \n",
    "    def pearson_correlation(x, y):\n",
    "        n = len(x)\n",
    "        # Calculate the mean of x and y\n",
    "        mean_x = sum(x) / n\n",
    "        mean_y = sum(y) / n\n",
    "        \n",
    "        # Calculate the numerator and denominators of the correlation coefficient\n",
    "        numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n",
    "        denominator_x = math.sqrt(sum((xi - mean_x) ** 2 for xi in x))\n",
    "        denominator_y = math.sqrt(sum((yi - mean_y) ** 2 for yi in y))\n",
    "        \n",
    "        # Calculate the correlation coefficient\n",
    "        correlation = numerator / (denominator_x * denominator_y)\n",
    "        return correlation\n",
    "        \n",
    "    corr_coefficient = pearson_correlation(predicted.flatten(), test_tensor)\n",
    "    corr_list.append(corr_coefficient)\n",
    "\n",
    "df_test_overall.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/LibA_wide_pivot_state3_test_predicted_cv10fold.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7951a63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_overall.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/LibA_wide_pivot_state3_test_predicted_cv10fold.csv\", index=False)\n",
    "df_test_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3096cd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def create_plots(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    \n",
    "create_plots(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
