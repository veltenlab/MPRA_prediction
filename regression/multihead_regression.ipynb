{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1f9e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/felix/anaconda3/envs/DeepFlyBrain_conda_env_gpu/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 237, 64)           6720      \n",
      "_________________________________________________________________\n",
      "average_pooling1d (AveragePo (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 48, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 39, 20)            12820     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 39, 20)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 1, 20)             0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 19,981\n",
      "Trainable params: 19,981\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Lambda, concatenate, Bidirectional, Dense, Dropout, Flatten, Conv1D,BatchNormalization,  MaxPooling1D, Bidirectional, GRU, TimeDistributed\n",
    "\n",
    "np.random.seed(1337) # for reproducibility\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "vocab = [\"A\", \"G\", \"C\", \"T\"]\n",
    "indices = tf.range(len(vocab), dtype = tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab,indices)\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, 1)\n",
    "defs = [0.] * 1 + [tf.constant([], dtype = \"string\")]\n",
    "\n",
    "def preprocess(record):\n",
    "    fields = tf.io.decode_csv(record, record_defaults=defs)\n",
    "    chars = tf.strings.bytes_split(fields[1])\n",
    "    chars_indeces = table.lookup(chars)\n",
    "    X = tf.one_hot(chars_indeces, depth = len(vocab))\n",
    "    Y = fields[0]\n",
    "    return X,Y\n",
    "\n",
    "def data_reader(file, batch_size=32, n_parse_threads = 4):\n",
    "    dataset = tf.data.TextLineDataset(file).skip(1)\n",
    "    dataset=dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n",
    "    \n",
    "\n",
    "inputs = Input(shape=(262,4), name=\"inputs\")\n",
    "layer = Conv1D(250, kernel_size=7, strides=1, activation='relu', name=\"conv1\")(inputs)  # 250 7 relu\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Conv1D(250, 8, strides=1, activation='softmax', name=\"conv2\")(layer)  # 250 8 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = MaxPooling1D(pool_size=2, strides=None, name=\"maxpool1\")(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Conv1D(250, 3, strides=1, activation='softmax', name=\"conv3\")(layer)  # 250 3 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Conv1D(100, 2, strides=1, activation='softmax', name=\"conv4\")(layer)  # 100 3 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = MaxPooling1D(pool_size=1, strides=None, name=\"maxpool2\")(layer)\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Flatten()(layer)\n",
    "layer = Dense(300, activation='sigmoid')(layer)  # 300\n",
    "layer = Dropout(0.5)(layer)\n",
    "layer = Dense(200, activation='sigmoid')(layer)  # 300\n",
    "predictions = Dense(1, activation='linear')(layer)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"mean_squared_error\",\n",
    "              metrics=[\"mse\", \"mae\", \"mape\"],\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0250f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nadav dataset\n",
    "import pandas as pd\n",
    "\n",
    "def data_reader(file, batch_size=100, n_parse_threads = 4):\n",
    "    dataset = tf.data.TextLineDataset(file).skip(1)\n",
    "    dataset=dataset.map(preprocess, num_parallel_calls = n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "def preprocess(record):\n",
    "    fields = tf.io.decode_csv(record, record_defaults=defs)\n",
    "    chars = tf.strings.bytes_split(fields[1])\n",
    "    chars_indeces = table.lookup(chars)\n",
    "    X = tf.one_hot(chars_indeces, depth = len(vocab))\n",
    "    Y = fields[0]\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "inputs = Input(shape=(200,4), name=\"inputs\")\n",
    "layer = Conv1D(250, kernel_size=7, strides=1, activation='relu', name=\"conv1\")(inputs)  # 250 7 relu\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Conv1D(250, 8, strides=1, activation='softmax', name=\"conv2\")(layer)  # 250 8 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = MaxPooling1D(pool_size=2, strides=None, name=\"maxpool1\")(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Conv1D(250, 3, strides=1, activation='softmax', name=\"conv3\")(layer)  # 250 3 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Conv1D(100, 2, strides=1, activation='softmax', name=\"conv4\")(layer)  # 100 3 softmax\n",
    "layer = BatchNormalization()(layer)\n",
    "layer = MaxPooling1D(pool_size=1, strides=None, name=\"maxpool2\")(layer)\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Flatten()(layer)\n",
    "layer = Dense(300, activation='sigmoid')(layer)  # 300\n",
    "layer = Dropout(0.3)(layer)\n",
    "layer = Dense(200, activation='sigmoid')(layer)  # 300\n",
    "predictions = Dense(1, activation='linear')(layer)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"mean_squared_error\",\n",
    "              metrics=[\"mse\", \"mae\", \"mape\"],\n",
    "              )\n",
    "\n",
    "histories = {}\n",
    "histories=model.fit(data_reader('/home/felix/cluster/fpacheco/Data/Nadav_lab/K562/mean_with_sequence_ENCFF616IAQ_2col_train.csv',batch_size=100),\n",
    "                        epochs=30,\n",
    "                        validation_data=data_reader('/home/felix/cluster/fpacheco/Data/Nadav_lab/K562/mean_with_sequence_ENCFF616IAQ_2col_validation.csv',batch_size=100),\n",
    "                        callbacks=None,\n",
    "                        verbose=2)\n",
    "\n",
    "predicted = model.predict(data_reader('/home/felix/cluster/fpacheco/Data/Nadav_lab/K562/mean_with_sequence_ENCFF616IAQ_2col_test.csv',\n",
    "                                            batch_size=100))\n",
    "\n",
    "test_data = data_reader('/home/felix/cluster/fpacheco/Data/Nadav_lab/K562/mean_with_sequence_ENCFF616IAQ_2col_test.csv',batch_size=100)\n",
    "test_tensor = X = np.empty(shape=[0,1])\n",
    "for batch in test_data:\n",
    "    test_tensor = np.append(test_tensor, batch[1])\n",
    "\n",
    "import math\n",
    "def pearson_correlation(x, y):\n",
    "    n = len(x)\n",
    "    # Calculate the mean of x and y\n",
    "    mean_x = sum(x) / n\n",
    "    mean_y = sum(y) / n\n",
    "    \n",
    "    # Calculate the numerator and denominators of the correlation coefficient\n",
    "    numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n",
    "    denominator_x = math.sqrt(sum((xi - mean_x) ** 2 for xi in x))\n",
    "    denominator_y = math.sqrt(sum((yi - mean_y) ** 2 for yi in y))\n",
    "    \n",
    "    # Calculate the correlation coefficient\n",
    "    correlation = numerator / (denominator_x * denominator_y)\n",
    "    return correlation\n",
    "    \n",
    "corr_coefficient = pearson_correlation(predicted.flatten(), test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a888a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "histories = {}\n",
    "for i in range(10):\n",
    "    ct=\"EoBaso\"\n",
    "    print(\"Fold %02d in %s\" % (i+1,ct))\n",
    "    histories[ct]=model.fit(data_reader(\"/home/felix/cluster/lvelten/Analysis/SCG4SYN/LibA_HSC/analysis/complete_run1_5cellstates/005_deep/data/%s/train%02d.csv\" % (ct, i+1),batch_size=32),\n",
    "                            epochs=30,\n",
    "                            validation_data=data_reader(\"/home/felix/cluster/lvelten/Analysis/SCG4SYN/LibA_HSC/analysis/complete_run1_5cellstates/005_deep/data/%s/test%02d.csv\" % (ct,i+1),batch_size=32),\n",
    "                            callbacks=None,\n",
    "                            verbose=2)\n",
    "    predicted = model.predict(data_reader(\"/home/felix/cluster/lvelten/Analysis/SCG4SYN/LibA_HSC/analysis/complete_run1_5cellstates/005_deep/data/%s/valid%02d.csv\" % (ct, i+1),\n",
    "                                              batch_size=100))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b903c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots(history):\n",
    "        plt.plot(history.history[\"pearson_r\"])\n",
    "        plt.plot(history.history[\"val_pearson_r\"])\n",
    "    else :\n",
    "        plt.plot(history.history[metric[4:]])\n",
    "        plt.plot(history.history[metric])\n",
    "    plt.title('model metric')\n",
    "    plt.ylabel(metric[4:])\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(foldername + 'metric.png')\n",
    "    plt.clf()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.savefig(foldername + 'loss.png')\n",
    "    plt.clf()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
