{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "921b4b40",
   "metadata": {},
   "source": [
    "# MPRA regression of background samples with K-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eb96b1",
   "metadata": {},
   "source": [
    "### Environment \n",
    "The next chunk contains the commands necessary to install the environment required to run this jupyter notebook\n",
    "Skip this chunk if the installation was previously done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76b192d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "conda create --name tf_MPRA python=3.9.7\n",
    "conda activate tf_MPRA\n",
    "pip install tensorflow[and-cuda]\n",
    "conda install -c anaconda ipykernel \n",
    "conda install -c anaconda pandas\n",
    "conda install -c anaconda numpy\n",
    "conda install -c anaconda scikit-learn \n",
    "conda install -c conda-forge matplotlib\n",
    "\n",
    "# After installation if you are using VSCODE to run the notebook you have to close it and re-open"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b66a7a",
   "metadata": {},
   "source": [
    "### Library imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1f9e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import getopt\n",
    "import sys\n",
    "\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import pandas as pd\n",
    "import math \n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Layer, Input, Masking, Lambda, concatenate, Bidirectional, Dense, Dropout, Flatten, Conv1D,BatchNormalization,  MaxPooling1D, Bidirectional, GRU, TimeDistributed\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37317fa",
   "metadata": {},
   "source": [
    "### Input ingestion\n",
    "\n",
    "Here we define the methods to read and ingest data and we initialize the random seed.\n",
    "\n",
    "Since we are processing the background the vocabulary is comprised of lower case nucleotides.\n",
    "\n",
    "The upper cases (where the motif is), will be encoded as a zero-like vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0aec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337) # for reproducibility\n",
    "\n",
    "# Lower case vocabulary\n",
    "vocab = [\"A\", \"C\", \"G\", \"T\"]\n",
    "indices = tf.range(len(vocab), dtype=tf.int64)\n",
    "table_init = tf.lookup.KeyValueTensorInitializer(vocab, indices)\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets=1)\n",
    "\n",
    "# These are the defaults of the data reader method \n",
    "# (each column in the ingested csv must be initialized with the right data type, otherwise the data ingestion fails )\n",
    "defs = [0.] * 1 + [tf.constant([], dtype=\"string\")] + [tf.constant([], dtype=\"string\")]\n",
    "\n",
    "\n",
    "def data_reader(file, batch_size=100, n_parse_threads=4):\n",
    "    \"\"\"Method for reading the data in an optimized way, can be used inside model.fit()\n",
    "    \n",
    "    Args:\n",
    "        file (_type_): path to csv file\n",
    "        batch_size (int, optional): _description_. Defaults to 100.\n",
    "        n_parse_threads (int, optional): _description_. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "        dataset.batch: batch dataset object \n",
    "    \"\"\"\n",
    "    dataset = tf.data.TextLineDataset(file).skip(1)\n",
    "    dataset = dataset.map(preprocess, num_parallel_calls=n_parse_threads)\n",
    "    return dataset.batch(batch_size).prefetch(1)\n",
    "\n",
    "def preprocess(record):\n",
    "    \"\"\"Preprocessing method of a dataset object, one-hot-encodes the data\n",
    "\n",
    "    Args:\n",
    "        record (_type_): _description_\n",
    "\n",
    "    Returns:\n",
    "        X (2D np.array): one-hot-encoded input sequence\n",
    "        Y (1D np.array): MPRA measurements\n",
    "\n",
    "    \"\"\"\n",
    "    # Extract fields from passed batch\n",
    "    fields = tf.io.decode_csv(record, record_defaults=defs)\n",
    "    chars = tf.strings.bytes_split(fields[1])     # Extract sequences from 1st field\n",
    "    chars_indices = table.lookup(chars)     # one-hot-encoding\n",
    "    \n",
    "    # Create a mask for out-of-vocabulary characters\n",
    "    oov_mask = tf.equal(chars_indices, len(vocab))\n",
    "    \n",
    "    # Explicitly cast the value to int64\n",
    "    chars_indices = tf.where(oov_mask, tf.constant(len(vocab), dtype=tf.int64), chars_indices)\n",
    "    \n",
    "    X = tf.one_hot(chars_indices, depth=len(vocab))\n",
    "    Y = fields[0]\n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6f55f",
   "metadata": {},
   "source": [
    "### Randomization of motif sequences and data augmentation\n",
    "\n",
    "This chunk defines a function that takes a dataframe with sequences and replaces upper case characters with random sequences N times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72098bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_motifs_and_augment(df, num_augmentations=100):\n",
    "    \"\"\"This methods takes a dataframe containing a 'seq' column, randomizes the upper case characters and augments this sequences n times\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe containing a 'seq' column\n",
    "        num_augmentations (int, optional): Number of random sequences to obtaining for each input sequence. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "        df (pd.DataFrame): df with an additional 'rnd_seq' column containing randomized upper case sequences, each sequence has N augmentation additional rows.\n",
    "    \"\"\"\n",
    "    augmented_sequences = []\n",
    "\n",
    "    for sequence in df['seq']:\n",
    "        for _ in range(num_augmentations):\n",
    "            random_sequence = ''.join(random.choice(\"ACGT\") if char.isupper() else char for char in sequence)\n",
    "            augmented_sequences.append(random_sequence)\n",
    "\n",
    "    augmented_df = pd.DataFrame({'rnd_seq': augmented_sequences})\n",
    "    df = df.loc[df.index.repeat(num_augmentations)].reset_index(drop=True)\n",
    "    df = pd.concat([df, augmented_df], axis=1)\n",
    "    return df\n",
    "\n",
    "# Example data frame with DNA sequences\n",
    "\n",
    "# test dataframe\n",
    "data = {'seq': [\"aaaACGTAGGCTA\", \"tttTTACGGTACACGT\", \"cccCGTACATACAGT\"],\n",
    "        'id' : [\"1a\", \"2a\", \"3a\"]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Call the randomize_motifs_and_augment function\n",
    "result_df = randomize_motifs_and_augment(df, num_augmentations=3)\n",
    "\n",
    "# Display the resulting data frame\n",
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4dae46",
   "metadata": {},
   "source": [
    "### k-fold cross validation split\n",
    "Here we take the initial csv file and we split it in 3 partitions k times\n",
    "\n",
    "It is possible to randomize the sequences and augment, since the masking of the model motifs was a better choice\n",
    "for understanding the background this strategy is here commented out and not used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6395a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CROSS VALIDATION (10 fold)\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "whole_data = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/no_upper_LibA_wide_pivot_state3.csv\"\n",
    "out_data = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/\"\n",
    "# Split the data in two partitions\n",
    "whole_data = pd.read_csv(whole_data)\n",
    "k = 10\n",
    "kf = KFold(n_splits = k, shuffle = True, random_state = 2008)\n",
    "\n",
    "o=1\n",
    "# For each fold we split again to get the third partition\n",
    "for i in kf.split(whole_data):\n",
    "    \n",
    "    # Extract data from fold\n",
    "    train = whole_data.iloc[i[0]]\n",
    "    test =  whole_data.iloc[i[1]]\n",
    "    \n",
    "    # Split into train, validation and test\n",
    "    valid, validation = train_test_split(train, test_size=0.111, random_state=42)\n",
    "    \n",
    "    # Randomize motifs\n",
    "    #train = randomize_motifs_and_augment(train, num_augmentations=100)\n",
    "    #train[\"rnd_seq\"] = train['rnd_seq'].str.lower() \n",
    "    #test = randomize_motifs_and_augment(test, num_augmentations=100)\n",
    "    #test[\"rnd_seq\"] = test['rnd_seq'].str.lower() \n",
    "    #validation = randomize_motifs_and_augment(validation, num_augmentations=100)\n",
    "    #validation[\"rnd_seq\"] = validation['rnd_seq'].str.lower() \n",
    "    \n",
    "    train[[\"State_3E\", \"seq\", \"CRS\"]].to_csv(out_data+\"background_CV\"+str(o)+\"_LibA_wide_pivot_state3_train.csv\", index=False)\n",
    "    test[[\"State_3E\", \"seq\", \"CRS\"]].to_csv(out_data+\"background_CV\"+str(o)+\"_LibA_wide_pivot_state3_test.csv\", index=False)\n",
    "    validation[[\"State_3E\", \"seq\", \"CRS\"]].to_csv(out_data+\"background_CV\"+str(o)+\"_LibA_wide_pivot_state3_validation.csv\", index=False)\n",
    "    o+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ad7e0d",
   "metadata": {},
   "source": [
    "### Masking motifs\n",
    "\n",
    "Since we do not want to backpropagate through the masked values we apply a mask on them\n",
    "\n",
    "The mask is applied on values one-hot-encoded as a zero-like vector\n",
    "\n",
    "Here we test the behaviour of the mask method to make sure it not masking 0 contained in every vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da889308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the masking layer with mask_value=0\n",
    "masking_layer = tf.keras.layers.Masking(mask_value=0.0)\n",
    "\n",
    "# Example input tensor with shape (batch_size, sequence_length, vocab_size)\n",
    "input_tensor = tf.constant([\n",
    "    [[0, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0]],\n",
    "    [[0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0]]\n",
    "], dtype=tf.float32)\n",
    "\n",
    "# Apply the masking layer to the input tensor\n",
    "masked_tensor = masking_layer(input_tensor)\n",
    "\n",
    "print(masked_tensor)\n",
    "masking_layer.compute_mask(input_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc417f4a",
   "metadata": {},
   "source": [
    "### Deep Learning model\n",
    "\n",
    "Here we run the model which is based on this paper : \n",
    "\n",
    "https://doi.org/10.1101/2023.03.05.531189\n",
    "\n",
    "I have added a Normalization layer parametrized with two parameters. Here we define the custom layer, the method to compute pearson correlation and a plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb5151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we initialize the df where each fold test prediction will be appended to\n",
    "# the list containing the correlations of each fold is also initialized\n",
    "df_test_10folds  = pd.DataFrame(columns=['State_3E', \"seq\", \"prediction\"])\n",
    "corr_list = []\n",
    "\n",
    "# We define a custom normalization layer to then compile on the model\n",
    "class CustomNormalization(Layer):\n",
    "    \"\"\"Custom normalization layer that normalizes the output of the neural network\"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        super(CustomNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        # Add trainable variables for mean and standard deviation\n",
    "        self.mean = self.add_weight(\"mean\", shape=(1,), initializer=\"zeros\", trainable=True)\n",
    "        self.stddev = self.add_weight(\"stddev\", shape=(1,), initializer=\"ones\", trainable=True)\n",
    "        super(CustomNormalization, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Normalize the inputs using the learned mean and standard deviation\n",
    "        return (inputs - self.mean) / (self.stddev + 1e-8)\n",
    "\n",
    "# We define the method to compute the pearson correlation between prediction and ground truth\n",
    "def pearson_correlation(x, y):\n",
    "    \"\"\"Computes Pearson Correlation between x and y\n",
    "    Args:\n",
    "        x (np.array): vector of predictions values\n",
    "        y (np.array): vector of ground truth values\n",
    "\n",
    "    Returns:\n",
    "        (float): pearson correlation\n",
    "    \"\"\"\n",
    "    n = len(x)\n",
    "    \n",
    "    # Calculate the mean of x and y\n",
    "    mean_x = sum(x) / n\n",
    "    mean_y = sum(y) / n\n",
    "    \n",
    "    # Calculate the numerator and denominators of the correlation coefficient\n",
    "    numerator = sum((xi - mean_x) * (yi - mean_y) for xi, yi in zip(x, y))\n",
    "    denominator_x = math.sqrt(sum((xi - mean_x) ** 2 for xi in x))\n",
    "    denominator_y = math.sqrt(sum((yi - mean_y) ** 2 for yi in y))\n",
    "    \n",
    "    # Calculate the correlation coefficient\n",
    "    correlation = numerator / (denominator_x * denominator_y)\n",
    "    return correlation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Define plotting function of loss\n",
    "def create_plots(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3faff54",
   "metadata": {},
   "source": [
    "#### Model training\n",
    "Here we iterate through the folds and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_10folds  = pd.DataFrame()\n",
    "corr_list = []\n",
    "\n",
    "for i in range(1,11):\n",
    "    #Define inputs\n",
    "    input_path_train = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/background_CV\"+str(i)+\"_LibA_wide_pivot_state3_train.csv\"\n",
    "    input_path_valid = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/background_CV\"+str(i)+\"_LibA_wide_pivot_state3_validation.csv\"\n",
    "    input_path_test = \"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/background_CV\"+str(i)+\"_LibA_wide_pivot_state3_test.csv\"\n",
    "   \n",
    "    df_test = pd.read_csv(input_path_test)\n",
    "    df_test[\"fold\"] = str(i)\n",
    "    corr_per_iteration = []\n",
    "    # Get first item of the dataset to get the shape of the input data\n",
    "    for element in data_reader(input_path_train):\n",
    "        input_shape = element[0].shape\n",
    "        \n",
    "    for iteration in range(1,11):\n",
    "        inputs = Input(shape=(input_shape[1],input_shape[2]), name=\"inputs\")\n",
    "        layer = Conv1D(250, kernel_size=7, strides=1, activation='relu', name=\"conv1\")(inputs)  # 250 7 relu\n",
    "        layer = Dropout(0.3)(layer)\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Conv1D(250, 8, strides=1, activation='softmax', name=\"conv2\")(layer)  # 250 8 softmax\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = MaxPooling1D(pool_size=2, strides=None, name=\"maxpool1\")(layer)\n",
    "        layer = Dropout(0.3)(layer)\n",
    "        layer = Conv1D(250, 3, strides=1, activation='softmax', name=\"conv3\")(layer)  # 250 3 softmax\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = Dropout(0.3)(layer)\n",
    "        layer = Conv1D(100, 2, strides=1, activation='softmax', name=\"conv4\")(layer)  # 100 3 softmax\n",
    "        layer = BatchNormalization()(layer)\n",
    "        layer = MaxPooling1D(pool_size=1, strides=None, name=\"maxpool2\")(layer)\n",
    "        layer = Dropout(0.3)(layer)\n",
    "        layer = Flatten()(layer)\n",
    "        layer = Dense(300, activation='sigmoid')(layer)  # 300\n",
    "        layer = Dropout(0.3)(layer)\n",
    "        layer = Dense(200, activation='sigmoid')(layer)  # 300\n",
    "        predictions = Dense(1, activation='linear')(layer)\n",
    "\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        model.compile(optimizer=\"adam\",\n",
    "                    loss=\"mean_squared_error\",\n",
    "                    metrics=[\"mse\", \"mae\", \"mape\"],\n",
    "                    )\n",
    "\n",
    "        history=model.fit(data_reader(input_path_train, batch_size=100),\n",
    "                                epochs=20,\n",
    "                                validation_data=data_reader(input_path_valid,batch_size=100),\n",
    "                                callbacks=None,\n",
    "                                verbose=2)\n",
    "\n",
    "        predicted = model.predict(data_reader(input_path_test,\n",
    "                                                    batch_size=100))\n",
    "\n",
    "        test_data = data_reader(input_path_test,batch_size=100)\n",
    "        test_tensor = X = np.empty(shape=[0,1])\n",
    "        for batch in test_data:\n",
    "            test_tensor = np.append(test_tensor, batch[1])\n",
    "\n",
    "        df_test[\"prediction_iteration_\"+str(iteration)] = predicted\n",
    "        \n",
    "                \n",
    "        corr_coefficient = pearson_correlation(predicted.flatten(), test_tensor)\n",
    "        corr_per_iteration.append(corr_coefficient)\n",
    "    \n",
    "    df_test_10folds = pd.concat([df_test_10folds, df_test], ignore_index=True)    \n",
    "        \n",
    "    corr_ensemble = np.mean(corr_per_iteration)\n",
    "    corr_list.append(corr_ensemble)\n",
    "\n",
    "df_test_10folds.to_csv(\"/home/felix/cluster/fpacheco/Data/Robert_data/processed_data/10fold_cv/LibA_wide_pivot_state3_test_predicted_cv10fold_motifs_only_ensemble.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2a2ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State_3E</th>\n",
       "      <th>seq</th>\n",
       "      <th>CRS</th>\n",
       "      <th>fold</th>\n",
       "      <th>prediction_iteration_1</th>\n",
       "      <th>prediction_iteration_2</th>\n",
       "      <th>prediction_iteration_3</th>\n",
       "      <th>prediction_iteration_4</th>\n",
       "      <th>prediction_iteration_5</th>\n",
       "      <th>prediction_iteration_6</th>\n",
       "      <th>prediction_iteration_7</th>\n",
       "      <th>prediction_iteration_8</th>\n",
       "      <th>prediction_iteration_9</th>\n",
       "      <th>prediction_iteration_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.005304</td>\n",
       "      <td>aggaccggatcaactaaacaactcaaacaagggctaatataaccca...</td>\n",
       "      <td>LibA.Seq7829</td>\n",
       "      <td>1</td>\n",
       "      <td>0.166644</td>\n",
       "      <td>0.008744</td>\n",
       "      <td>0.079746</td>\n",
       "      <td>0.205307</td>\n",
       "      <td>0.077799</td>\n",
       "      <td>0.092063</td>\n",
       "      <td>0.154507</td>\n",
       "      <td>0.082586</td>\n",
       "      <td>0.092175</td>\n",
       "      <td>0.120026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.236881</td>\n",
       "      <td>aggaccggatcaactaaacactagtcatacttaaaaattgcaagga...</td>\n",
       "      <td>LibA.Seq271</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009147</td>\n",
       "      <td>-0.044254</td>\n",
       "      <td>0.058593</td>\n",
       "      <td>-0.053590</td>\n",
       "      <td>0.069522</td>\n",
       "      <td>0.017253</td>\n",
       "      <td>-0.004010</td>\n",
       "      <td>0.030977</td>\n",
       "      <td>0.016632</td>\n",
       "      <td>0.065043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.086491</td>\n",
       "      <td>aggaccggatcaactaaacaggttctgacgtatgctcctctatgga...</td>\n",
       "      <td>LibA.Seq4548</td>\n",
       "      <td>1</td>\n",
       "      <td>0.019830</td>\n",
       "      <td>-0.027810</td>\n",
       "      <td>0.022879</td>\n",
       "      <td>0.063743</td>\n",
       "      <td>0.031034</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>0.017735</td>\n",
       "      <td>0.037538</td>\n",
       "      <td>0.060172</td>\n",
       "      <td>0.028070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.087684</td>\n",
       "      <td>aggaccggatcaactaaacccgagcctgcctagccctagcttctct...</td>\n",
       "      <td>LibA.Seq4582</td>\n",
       "      <td>1</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>0.004229</td>\n",
       "      <td>0.022408</td>\n",
       "      <td>0.027580</td>\n",
       "      <td>0.063660</td>\n",
       "      <td>0.021920</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>0.037809</td>\n",
       "      <td>0.055539</td>\n",
       "      <td>0.026170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.371111</td>\n",
       "      <td>aggaccggatcaactaaacggagcagagttagtgtcaggtcaaaaa...</td>\n",
       "      <td>LibA.Seq2863</td>\n",
       "      <td>1</td>\n",
       "      <td>0.177948</td>\n",
       "      <td>0.104138</td>\n",
       "      <td>0.251513</td>\n",
       "      <td>0.305862</td>\n",
       "      <td>0.222175</td>\n",
       "      <td>0.221340</td>\n",
       "      <td>0.207982</td>\n",
       "      <td>0.224182</td>\n",
       "      <td>0.204155</td>\n",
       "      <td>0.198397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8487</th>\n",
       "      <td>-0.076574</td>\n",
       "      <td>aggaccggatcaacttttggtcggttgacggtcgccttgattattc...</td>\n",
       "      <td>LibA.Seq4154</td>\n",
       "      <td>10</td>\n",
       "      <td>0.025850</td>\n",
       "      <td>-0.119319</td>\n",
       "      <td>0.059446</td>\n",
       "      <td>0.065505</td>\n",
       "      <td>-0.003330</td>\n",
       "      <td>0.032118</td>\n",
       "      <td>0.084711</td>\n",
       "      <td>0.060746</td>\n",
       "      <td>0.052425</td>\n",
       "      <td>0.102655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8488</th>\n",
       "      <td>0.369648</td>\n",
       "      <td>aggaccggatcaactttttcagtgaaagatcaccgcgggatctcac...</td>\n",
       "      <td>LibA.Seq8531</td>\n",
       "      <td>10</td>\n",
       "      <td>0.471889</td>\n",
       "      <td>0.022261</td>\n",
       "      <td>0.221582</td>\n",
       "      <td>0.396499</td>\n",
       "      <td>0.270562</td>\n",
       "      <td>0.340061</td>\n",
       "      <td>0.331227</td>\n",
       "      <td>0.339184</td>\n",
       "      <td>0.368296</td>\n",
       "      <td>0.206447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8489</th>\n",
       "      <td>-0.098525</td>\n",
       "      <td>aggaccggatcaacttttttagtaaaactcttaaacagtgattaca...</td>\n",
       "      <td>LibA.Seq6744</td>\n",
       "      <td>10</td>\n",
       "      <td>0.213882</td>\n",
       "      <td>-0.037019</td>\n",
       "      <td>0.103241</td>\n",
       "      <td>0.078904</td>\n",
       "      <td>0.061012</td>\n",
       "      <td>0.094831</td>\n",
       "      <td>0.164944</td>\n",
       "      <td>0.172287</td>\n",
       "      <td>0.074359</td>\n",
       "      <td>0.275276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8490</th>\n",
       "      <td>0.164442</td>\n",
       "      <td>aggaccggatcaacttttttatctggttatcattctagtctagtgc...</td>\n",
       "      <td>LibA.Seq1298</td>\n",
       "      <td>10</td>\n",
       "      <td>0.154465</td>\n",
       "      <td>-0.016604</td>\n",
       "      <td>0.095095</td>\n",
       "      <td>0.145902</td>\n",
       "      <td>0.082548</td>\n",
       "      <td>0.228232</td>\n",
       "      <td>0.094881</td>\n",
       "      <td>0.129145</td>\n",
       "      <td>0.094053</td>\n",
       "      <td>0.110160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8491</th>\n",
       "      <td>0.352354</td>\n",
       "      <td>aggaccggatcaacttttttccccgtctgccaacttcgtggctatc...</td>\n",
       "      <td>LibA.Seq908</td>\n",
       "      <td>10</td>\n",
       "      <td>0.230760</td>\n",
       "      <td>0.132370</td>\n",
       "      <td>0.193284</td>\n",
       "      <td>0.238966</td>\n",
       "      <td>0.200822</td>\n",
       "      <td>0.284851</td>\n",
       "      <td>0.221795</td>\n",
       "      <td>0.189083</td>\n",
       "      <td>0.262462</td>\n",
       "      <td>0.162159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8492 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      State_3E                                                seq  \\\n",
       "0    -0.005304  aggaccggatcaactaaacaactcaaacaagggctaatataaccca...   \n",
       "1     0.236881  aggaccggatcaactaaacactagtcatacttaaaaattgcaagga...   \n",
       "2    -0.086491  aggaccggatcaactaaacaggttctgacgtatgctcctctatgga...   \n",
       "3    -0.087684  aggaccggatcaactaaacccgagcctgcctagccctagcttctct...   \n",
       "4     0.371111  aggaccggatcaactaaacggagcagagttagtgtcaggtcaaaaa...   \n",
       "...        ...                                                ...   \n",
       "8487 -0.076574  aggaccggatcaacttttggtcggttgacggtcgccttgattattc...   \n",
       "8488  0.369648  aggaccggatcaactttttcagtgaaagatcaccgcgggatctcac...   \n",
       "8489 -0.098525  aggaccggatcaacttttttagtaaaactcttaaacagtgattaca...   \n",
       "8490  0.164442  aggaccggatcaacttttttatctggttatcattctagtctagtgc...   \n",
       "8491  0.352354  aggaccggatcaacttttttccccgtctgccaacttcgtggctatc...   \n",
       "\n",
       "               CRS fold  prediction_iteration_1  prediction_iteration_2  \\\n",
       "0     LibA.Seq7829    1                0.166644                0.008744   \n",
       "1      LibA.Seq271    1                0.009147               -0.044254   \n",
       "2     LibA.Seq4548    1                0.019830               -0.027810   \n",
       "3     LibA.Seq4582    1                0.005316                0.004229   \n",
       "4     LibA.Seq2863    1                0.177948                0.104138   \n",
       "...            ...  ...                     ...                     ...   \n",
       "8487  LibA.Seq4154   10                0.025850               -0.119319   \n",
       "8488  LibA.Seq8531   10                0.471889                0.022261   \n",
       "8489  LibA.Seq6744   10                0.213882               -0.037019   \n",
       "8490  LibA.Seq1298   10                0.154465               -0.016604   \n",
       "8491   LibA.Seq908   10                0.230760                0.132370   \n",
       "\n",
       "      prediction_iteration_3  prediction_iteration_4  prediction_iteration_5  \\\n",
       "0                   0.079746                0.205307                0.077799   \n",
       "1                   0.058593               -0.053590                0.069522   \n",
       "2                   0.022879                0.063743                0.031034   \n",
       "3                   0.022408                0.027580                0.063660   \n",
       "4                   0.251513                0.305862                0.222175   \n",
       "...                      ...                     ...                     ...   \n",
       "8487                0.059446                0.065505               -0.003330   \n",
       "8488                0.221582                0.396499                0.270562   \n",
       "8489                0.103241                0.078904                0.061012   \n",
       "8490                0.095095                0.145902                0.082548   \n",
       "8491                0.193284                0.238966                0.200822   \n",
       "\n",
       "      prediction_iteration_6  prediction_iteration_7  prediction_iteration_8  \\\n",
       "0                   0.092063                0.154507                0.082586   \n",
       "1                   0.017253               -0.004010                0.030977   \n",
       "2                   0.008034                0.017735                0.037538   \n",
       "3                   0.021920                0.009619                0.037809   \n",
       "4                   0.221340                0.207982                0.224182   \n",
       "...                      ...                     ...                     ...   \n",
       "8487                0.032118                0.084711                0.060746   \n",
       "8488                0.340061                0.331227                0.339184   \n",
       "8489                0.094831                0.164944                0.172287   \n",
       "8490                0.228232                0.094881                0.129145   \n",
       "8491                0.284851                0.221795                0.189083   \n",
       "\n",
       "      prediction_iteration_9  prediction_iteration_10  \n",
       "0                   0.092175                 0.120026  \n",
       "1                   0.016632                 0.065043  \n",
       "2                   0.060172                 0.028070  \n",
       "3                   0.055539                 0.026170  \n",
       "4                   0.204155                 0.198397  \n",
       "...                      ...                      ...  \n",
       "8487                0.052425                 0.102655  \n",
       "8488                0.368296                 0.206447  \n",
       "8489                0.074359                 0.275276  \n",
       "8490                0.094053                 0.110160  \n",
       "8491                0.262462                 0.162159  \n",
       "\n",
       "[8492 rows x 14 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_10folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7772ea68",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_plots(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
